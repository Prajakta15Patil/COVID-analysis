{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Flatten, Concatenate, TimeDistributed, Bidirectional, Attention, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import TensorShape\n",
    "import tensorflow_addons as tfa\n",
    "from langdetect import detect\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 1.88 s, total: 15.3 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "path = '/home/ubuntu/data/*.json'\n",
    "files = glob.glob(path)\n",
    "papers = []\n",
    "for file in files:\n",
    "    with open(file) as json_file:\n",
    "            text = json.load(json_file)\n",
    "            papers.append([text['paper_id'], text['bodytext'], text['abstract']])\n",
    "data = pd.DataFrame(papers, columns = ['paper_id', 'bodytext', 'abstract'])\n",
    "\n",
    "#get the lengths of texts\n",
    "data['len_bt'] = data.bodytext.map(lambda x: len(x.split(\" \")))\n",
    "data['len_ab'] = data.abstract.map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#filter papers with certain word length\n",
    "data.query('len_bt <= 1000 and len_bt>=100 and len_ab <= 500 and len_ab >=20', inplace = True)\n",
    "\n",
    "#detect languages of texts to filter out non-english papers\n",
    "data['bt_lang'] = data.bodytext.map(lambda x: detect(x))\n",
    "data['ab_lang'] = data.abstract.map(lambda x: detect(x))\n",
    "\n",
    "#use only english papers\n",
    "data = data[(data['bt_lang'] == 'en') & (data['ab_lang'] == 'en')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_bt</th>\n",
       "      <th>len_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>183.000000</td>\n",
       "      <td>183.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>586.874317</td>\n",
       "      <td>117.213115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>236.601326</td>\n",
       "      <td>92.766891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>419.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>557.000000</td>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>815.000000</td>\n",
       "      <td>156.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>500.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           len_bt      len_ab\n",
       "count  183.000000  183.000000\n",
       "mean   586.874317  117.213115\n",
       "std    236.601326   92.766891\n",
       "min    101.000000   28.000000\n",
       "25%    419.000000   51.000000\n",
       "50%    557.000000   84.000000\n",
       "75%    815.000000  156.000000\n",
       "max    999.000000  500.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Reduce vocab_size\n",
    "# start with smaller sample. 1000/2000\n",
    "# truncate the papers to certain word size\n",
    "# remove words with a certain frequency. start with 2000 words. \n",
    "# remove words from bodytext and feed it to the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f16816a9890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data.len_bt,data.len_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(bodytext):\n",
    "    cleaned = list()\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for word in bodytext:\n",
    "        words = str(word)       \n",
    "        words = words.lower()\n",
    "        words = words.translate(table)\n",
    "        words = re_print.sub('', words) \n",
    "        if words.isalpha() == True:\n",
    "            cleaned.append(words)\n",
    "    cleaned.insert(0, '<start>')\n",
    "    cleaned.append('<end>')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30 s, sys: 615 ms, total: 30.6 s\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "bt_vector = list()\n",
    "bt_list = []\n",
    "ab_list = []\n",
    "for i in range(len(data)):\n",
    "    bodytext = nlp(data.iloc[i].bodytext)\n",
    "    bt_clean = clean_text(bodytext)\n",
    "    bt_list.append(bt_clean)\n",
    "    \n",
    "    abstract = nlp(data.iloc[i].abstract)\n",
    "    ab_clean = clean_text(abstract)\n",
    "    ab_list.append(ab_clean)\n",
    "com_list = ab_list + bt_list\n",
    "    #c_papers.append(papers)\n",
    "bt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "bt_tokenizer.fit_on_texts(com_list)\n",
    "data_bt = bt_tokenizer.texts_to_sequences(bt_list)\n",
    "data_ab = bt_tokenizer.texts_to_sequences(ab_list)\n",
    "\n",
    "longest_seq = max(max([len(x) for x in data_bt]), max([len(x) for x in data_ab]))\n",
    "#x_voc_size = max([len(x) for x in data_bt])#, max([len(x) for x in data_ab]))\n",
    "#y_voc_size = max([len(y) for y in data_ab])\n",
    "data_bt = tf.keras.preprocessing.sequence.pad_sequences(data_bt,padding='post', maxlen = longest_seq)\n",
    "data_ab = tf.keras.preprocessing.sequence.pad_sequences(data_ab,padding='post', maxlen = longest_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(tensor):\n",
    "    #print( np.argmax([len(t) for t in tensor]))\n",
    "    return max( len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(data_bt,data_ab,test_size=0.2)\n",
    "BATCH_SIZE = 50\n",
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 256\n",
    "rnn_units = 128\n",
    "dense_units = 128\n",
    "Dtype = tf.float32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get abstracts no longer than 100 words, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = bt_tokenizer.word_index\n",
    "print(list(a.keys())[list(a.values()).index(9326)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9684"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(bt_tokenizer.word_index)+1  \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  27,  535,    3, ...,    0,    0,    0],\n",
       "       [  27,  365,   75, ...,    0,    0,    0],\n",
       "       [  27,    1,  403, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  27,  229, 7968, ...,    0,    0,    0],\n",
       "       [  27, 1377,   11, ...,    0,    0,    0],\n",
       "       [  27,    6,  171, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_initial_state():\n",
    "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n",
    "encoder_initial_cell_state = initialize_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "class EncoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
    "                                                           output_dim=embedding_dims)\n",
    "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
    "                                                     return_state=True )\n",
    "    \n",
    "#DECODER\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
    "        super().__init__()\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
    "                                                           output_dim=embedding_dims) \n",
    "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[longest_seq])\n",
    "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
    "                                                output_layer=self.dense_layer)\n",
    "\n",
    "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
    "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
    "                                          memory_sequence_length=memory_sequence_length)\n",
    "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    # wrap decodernn cell  \n",
    "    def build_rnn_cell(self, batch_size ):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
    "                                                attention_layer_size=dense_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
    "                                                                dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "        return decoder_initial_state\n",
    "\n",
    "encoderNetwork = EncoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "decoderNetwork = DecoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.EncoderNetwork object at 0x7f1691133950>, because it is not built.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/gpu/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./covid_encoder/assets\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.DecoderNetwork object at 0x7f16163d2790>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./covid_decoder/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(encoderNetwork, \"./covid_encoder\")\n",
    "tf.saved_model.save(decoderNetwork, \"./covid_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9684, 959, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, longest_seq, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y):\n",
    "   \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "\n",
    "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
    "        \n",
    "         \n",
    "        # Prepare correct Decoder input & output sequence data\n",
    "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "        #compare logits with timestepped +1 version of decoder_input\n",
    "        decoder_output = output_batch[:,1:] #ignore <start>\n",
    "\n",
    "\n",
    "        # Decoder Embeddings\n",
    "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "        \n",
    "        #BasicDecoderOutput        \n",
    "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                               sequence_length=BATCH_SIZE*[longest_seq-1])\n",
    "\n",
    "        logits = outputs.rnn_output\n",
    "        #Calculate loss\n",
    "\n",
    "        loss = loss_function(logits, decoder_output)\n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for i in range(1, epochs+1):\n",
    "\n",
    "    encoder_initial_cell_state = initialize_initial_state()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
    "        total_loss += batch_loss\n",
    "        if (batch+1)%5 == 0:\n",
    "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(50, 959, 256), dtype=float32, numpy=\n",
       "array([[[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [ 0.02633841, -0.04571971,  0.04504414, ..., -0.01970328,\n",
       "         -0.03171159,  0.03187238],\n",
       "        [-0.00557921, -0.00554545, -0.04175382, ..., -0.02747589,\n",
       "         -0.03312651,  0.02503815],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]],\n",
       "\n",
       "       [[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [-0.04813649,  0.01817832, -0.02858244, ..., -0.00870688,\n",
       "          0.04454856, -0.04202262],\n",
       "        [ 0.00587633, -0.03932049, -0.04086987, ..., -0.01227671,\n",
       "         -0.00820951,  0.04256004],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]],\n",
       "\n",
       "       [[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [-0.0201616 , -0.0186884 ,  0.02857475, ...,  0.00858632,\n",
       "         -0.03932483,  0.0362366 ],\n",
       "        [-0.02009363, -0.00568918, -0.02341797, ...,  0.01236438,\n",
       "         -0.0338526 , -0.00151773],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [-0.03948962, -0.02185796,  0.00898256, ..., -0.00232921,\n",
       "         -0.00639926,  0.03640429],\n",
       "        [ 0.02260518, -0.00480072,  0.01115143, ..., -0.04106044,\n",
       "         -0.0256431 , -0.04100025],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]],\n",
       "\n",
       "       [[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [ 0.03932607, -0.0282916 ,  0.01337966, ..., -0.00278549,\n",
       "          0.01858065,  0.00535438],\n",
       "        [-0.04197508, -0.01446979,  0.00939658, ...,  0.02029252,\n",
       "          0.02875839,  0.0170986 ],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]],\n",
       "\n",
       "       [[-0.00991693, -0.01446358,  0.02853112, ..., -0.0430769 ,\n",
       "         -0.04316779,  0.0106885 ],\n",
       "        [ 0.0097247 ,  0.02648927,  0.00349119, ..., -0.02793862,\n",
       "          0.02925212,  0.03635189],\n",
       "        [-0.01020823,  0.0224261 , -0.00569185, ...,  0.02106522,\n",
       "         -0.00246119,  0.00392824],\n",
       "        ...,\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ],\n",
       "        [ 0.00774547, -0.03388133,  0.040099  , ...,  0.04583717,\n",
       "          0.03554945,  0.046782  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "encoder_emb_inp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_raw= X_test[0]\n",
    "\n",
    "#inp = tf.convert_to_tensor(input_raw)\n",
    "encoder_initial_cell_state1 = [tf.zeros((50, rnn_units)),\n",
    "                             tf.zeros((50, rnn_units))]\n",
    "encoder_emb_inp1 = encoderNetwork.encoder_embedding(input_raw)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp1,\n",
    "                                               initial_state =encoder_initial_cell_state1)\n",
    "\n",
    "# print('a_tx :', a_tx.shape)\n",
    "# print('c_tx :', c_tx.shape)\n",
    "encoder_emb_inp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 959)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_raw.reshape(1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
    "#through the length of the model, for this we use greedsampler to run through the decoder\n",
    "#and the final embedding matrix trained on the data is used to generate embeddings\n",
    "input_raw=X_test[0].reshape(1,-1)\n",
    "# inp_bodytext = nlp(input_raw)\n",
    "# input_lines = clean_text(inp_bodytext)\n",
    "\n",
    "# We have a transcript file containing English-German pairs\n",
    "# Preprocess X\n",
    "#input_raw = clean_text(input_raw)\n",
    "#input_lines = [f'{bt_tok} {input_raw}']\n",
    "# input_sequences = [[bt_tokenizer.word_index[w] for w in line.split()] for line in input_raw]\n",
    "# input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_raw,\n",
    "#                                                                maxlen=longest_seq, padding='post')\n",
    "# inp = tf.convert_to_tensor(input_sequences)\n",
    "\n",
    "#print(\"inp\", inp.shape)\n",
    "#print(\"inp_seq\",input_sequences)\n",
    "# inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((1, rnn_units)),tf.zeros((1, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(input_raw)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,initial_state =encoder_initial_cell_state)\n",
    "print('a_tx :', a_tx.shape)\n",
    "print('c_tx :', c_tx.shape)\n",
    "\n",
    "start_tokens = tf.fill([1],bt_tokenizer.word_index['<start>'])\n",
    "\n",
    "end_token = bt_tokenizer.word_index['<end>']\n",
    "\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "decoder_input = tf.expand_dims([bt_tokenizer.word_index['<start>']]* 1,1)\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoderNetwork.dense_layer)\n",
    "decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "print(f\"decoder_initial_state = [a_tx, c_tx] : {np.array([a_tx, c_tx]).shape}\")\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(1,\n",
    "                                                                   encoder_state=[a_tx, c_tx],\n",
    "                                                                   Dtype=tf.float32)\n",
    "print(f\"\"\"\n",
    "Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
    "is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
    "\n",
    "decoder initial state shape: {np.array(decoder_initial_state).shape}\n",
    "decoder_initial_state tensor\n",
    "{decoder_initial_state}\n",
    "\"\"\")\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(longest_seq) * 2)\n",
    "\n",
    "#initialize inference decoder\n",
    "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "print(f\"first_inputs returns the same decoder_input i.e. embedding of  {'<start>'} : {first_inputs.shape}\")\n",
    "print(f\"start_index_emb_avg {tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))}\") # mean along the batch\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state  \n",
    "predictions = np.empty((1,0), dtype = np.int32)                                                                             \n",
    "for j in range(maximum_iterations):\n",
    "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary:\")\n",
    "for i in range(len(predictions)):\n",
    "    line = predictions[i,:]\n",
    "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
    "    print(\" \".join( [bt_tokenizer.index_word[w] for w in seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
