{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import itertools\n",
    "from pickle import load\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_documnet(filename):\n",
    "# open the file as read only\n",
    "  file = open(filename, mode='rt', encoding='utf-8')\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def doc_sep_pair(doc):\n",
    "  lines = doc.strip().split('\\n')\n",
    "  pairs = [line.split('\\t') for line in  lines]\n",
    "  return pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_sentences(lines):\n",
    "  cleaned = list()\n",
    "  re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "  # prepare translation table \n",
    "  table = str.maketrans('', '', string.punctuation)\n",
    "  for pair in lines:\n",
    "    clean_pair = list()\n",
    "    for line in pair:\n",
    "      # normalizing unicode characters\n",
    "      line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "      line = line.decode('UTF-8')\n",
    "      # tokenize on white space\n",
    "      line = line.split()\n",
    "      # convert to lowercase\n",
    "      line = [word.lower() for word in line]\n",
    "      # removing punctuation\n",
    "      line = [word.translate(table) for word in line]\n",
    "      # removing non-printable chars form each token\n",
    "      line = [re_print.sub('', w) for w in line]\n",
    "      # removing tokens with numbers\n",
    "      line = [word for word in line if word.isalpha()]\n",
    "\n",
    "      line.insert(0,'<start> ')\n",
    "      line.append(' <end>')\n",
    "      # store as string\n",
    "      clean_pair.append(' '.join(line))\n",
    "    cleaned.append(clean_pair)\n",
    "  return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize('NFD', pairs[0][2]).encode('ascii', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "filename = '/home/prajakta/deu.txt' #change filename if necessary\n",
    "doc = load_documnet(filename)\n",
    "\n",
    "#clean sentences and save clean data\n",
    "pairs = doc_sep_pair(doc)\n",
    "clean_sentences = clean_sentences(pairs)\n",
    "raw_data = clean_sentences\n",
    "data = raw_data[:10000, :2] \n",
    "import numpy as np\n",
    "raw_data_en = list()\n",
    "raw_data_ge = list()\n",
    "for data1 in data:\n",
    "  raw_data_en.append(data1[0]),raw_data_ge.append(data1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n",
    "\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding='post')\n",
    "\n",
    "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
    "\n",
    "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
    "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(tensor):\n",
    "    #print( np.argmax([len(t) for t in tensor]))\n",
    "    return max( len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size=0.2)\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 256\n",
    "rnn_units = 1024\n",
    "dense_units = 1024\n",
    "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = max_len(data_en)\n",
    "Ty = max_len(data_ge)  \n",
    "\n",
    "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
    "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "example_X, example_Y = next(iter(dataset))\n",
    "#print(example_X.shape) \n",
    "#print(example_Y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "class EncoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
    "                                                           output_dim=embedding_dims)\n",
    "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
    "                                                     return_state=True )\n",
    "    \n",
    "#DECODER\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
    "        super().__init__()\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
    "                                                           output_dim=embedding_dims) \n",
    "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
    "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
    "                                                output_layer=self.dense_layer)\n",
    "\n",
    "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
    "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
    "                                          memory_sequence_length=memory_sequence_length)\n",
    "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    # wrap decodernn cell  \n",
    "    def build_rnn_cell(self, batch_size ):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
    "                                                attention_layer_size=dense_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
    "                                                                dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "        return decoder_initial_state\n",
    "\n",
    "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
    "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y):\n",
    "   \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "\n",
    "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
    "        \n",
    "         \n",
    "        # Prepare correct Decoder input & output sequence data\n",
    "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "        #compare logits with timestepped +1 version of decoder_input\n",
    "        decoder_output = output_batch[:,1:] #ignore <start>\n",
    "\n",
    "\n",
    "        # Decoder Embeddings\n",
    "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "        \n",
    "        #BasicDecoderOutput        \n",
    "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
    "\n",
    "        logits = outputs.rnn_output\n",
    "        #Calculate loss\n",
    "\n",
    "        loss = loss_function(logits, decoder_output)\n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 7, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10, 3579)\n"
     ]
    }
   ],
   "source": [
    "decoder_input = output_batch[:,:-1]\n",
    "decoder_output = output_batch[:,1:] \n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state, sequence_length=BATCH_SIZE*[Ty-1])\n",
    "logits = outputs.rnn_output\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n",
      "(64, 7)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_output.shape)\n",
    "print(input_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN LSTM hidden and memory state initializer\n",
    "def initialize_initial_state():\n",
    "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 1024), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((BATCH_SIZE, rnn_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 3.7930893898010254 epoch 1 batch 5 \n",
      "total loss: 2.5809988975524902 epoch 1 batch 10 \n",
      "total loss: 2.4079675674438477 epoch 1 batch 15 \n",
      "total loss: 2.2165372371673584 epoch 1 batch 20 \n",
      "total loss: 2.1646690368652344 epoch 1 batch 25 \n",
      "total loss: 2.153352737426758 epoch 1 batch 30 \n",
      "total loss: 2.0302047729492188 epoch 1 batch 35 \n",
      "total loss: 2.0439178943634033 epoch 1 batch 40 \n",
      "total loss: 1.9471286535263062 epoch 1 batch 45 \n",
      "total loss: 1.9588428735733032 epoch 1 batch 50 \n",
      "total loss: 1.817651391029358 epoch 1 batch 55 \n",
      "total loss: 2.0174901485443115 epoch 1 batch 60 \n",
      "total loss: 1.9455722570419312 epoch 1 batch 65 \n",
      "total loss: 1.8384926319122314 epoch 1 batch 70 \n",
      "total loss: 1.911935806274414 epoch 1 batch 75 \n",
      "total loss: 1.848687767982483 epoch 1 batch 80 \n",
      "total loss: 1.9593467712402344 epoch 1 batch 85 \n",
      "total loss: 1.741815209388733 epoch 1 batch 90 \n",
      "total loss: 1.7861419916152954 epoch 1 batch 95 \n",
      "total loss: 1.8733680248260498 epoch 1 batch 100 \n",
      "total loss: 1.8702430725097656 epoch 1 batch 105 \n",
      "total loss: 1.757742166519165 epoch 1 batch 110 \n",
      "total loss: 1.6838958263397217 epoch 1 batch 115 \n",
      "total loss: 1.7754151821136475 epoch 1 batch 120 \n",
      "total loss: 1.677456259727478 epoch 1 batch 125 \n",
      "total loss: 1.595044732093811 epoch 2 batch 5 \n",
      "total loss: 1.5659139156341553 epoch 2 batch 10 \n",
      "total loss: 1.6101408004760742 epoch 2 batch 15 \n",
      "total loss: 1.62359619140625 epoch 2 batch 20 \n",
      "total loss: 1.6468846797943115 epoch 2 batch 25 \n",
      "total loss: 1.6647030115127563 epoch 2 batch 30 \n",
      "total loss: 1.5887387990951538 epoch 2 batch 35 \n",
      "total loss: 1.6783199310302734 epoch 2 batch 40 \n",
      "total loss: 1.5065028667449951 epoch 2 batch 45 \n",
      "total loss: 1.6208003759384155 epoch 2 batch 50 \n",
      "total loss: 1.7087299823760986 epoch 2 batch 55 \n",
      "total loss: 1.4709395170211792 epoch 2 batch 60 \n",
      "total loss: 1.6743946075439453 epoch 2 batch 65 \n",
      "total loss: 1.5098885297775269 epoch 2 batch 70 \n",
      "total loss: 1.6124866008758545 epoch 2 batch 75 \n",
      "total loss: 1.538090705871582 epoch 2 batch 80 \n",
      "total loss: 1.4166759252548218 epoch 2 batch 85 \n",
      "total loss: 1.6592013835906982 epoch 2 batch 90 \n",
      "total loss: 1.5475190877914429 epoch 2 batch 95 \n",
      "total loss: 1.284646987915039 epoch 2 batch 100 \n",
      "total loss: 1.379880666732788 epoch 2 batch 105 \n",
      "total loss: 1.5032743215560913 epoch 2 batch 110 \n",
      "total loss: 1.429893136024475 epoch 2 batch 115 \n",
      "total loss: 1.4010804891586304 epoch 2 batch 120 \n",
      "total loss: 1.4060159921646118 epoch 2 batch 125 \n",
      "total loss: 1.309484601020813 epoch 3 batch 5 \n",
      "total loss: 1.2994791269302368 epoch 3 batch 10 \n",
      "total loss: 1.5330249071121216 epoch 3 batch 15 \n",
      "total loss: 1.420715093612671 epoch 3 batch 20 \n",
      "total loss: 1.2901090383529663 epoch 3 batch 25 \n",
      "total loss: 1.3297284841537476 epoch 3 batch 30 \n",
      "total loss: 1.5098698139190674 epoch 3 batch 35 \n",
      "total loss: 1.4474904537200928 epoch 3 batch 40 \n",
      "total loss: 1.410517930984497 epoch 3 batch 45 \n",
      "total loss: 1.3324048519134521 epoch 3 batch 50 \n",
      "total loss: 1.3584730625152588 epoch 3 batch 55 \n",
      "total loss: 1.361868143081665 epoch 3 batch 60 \n",
      "total loss: 1.3909356594085693 epoch 3 batch 65 \n",
      "total loss: 1.4039978981018066 epoch 3 batch 70 \n",
      "total loss: 1.3138936758041382 epoch 3 batch 75 \n",
      "total loss: 1.3420137166976929 epoch 3 batch 80 \n",
      "total loss: 1.3527185916900635 epoch 3 batch 85 \n",
      "total loss: 1.2270665168762207 epoch 3 batch 90 \n",
      "total loss: 1.2649562358856201 epoch 3 batch 95 \n",
      "total loss: 1.2243741750717163 epoch 3 batch 100 \n",
      "total loss: 1.4344663619995117 epoch 3 batch 105 \n",
      "total loss: 1.4764182567596436 epoch 3 batch 110 \n",
      "total loss: 1.3603845834732056 epoch 3 batch 115 \n",
      "total loss: 1.227431058883667 epoch 3 batch 120 \n",
      "total loss: 1.265886664390564 epoch 3 batch 125 \n",
      "total loss: 1.1233810186386108 epoch 4 batch 5 \n",
      "total loss: 1.1678645610809326 epoch 4 batch 10 \n",
      "total loss: 1.1339595317840576 epoch 4 batch 15 \n",
      "total loss: 1.123750925064087 epoch 4 batch 20 \n",
      "total loss: 1.1083905696868896 epoch 4 batch 25 \n",
      "total loss: 1.1000523567199707 epoch 4 batch 30 \n",
      "total loss: 1.1496937274932861 epoch 4 batch 35 \n",
      "total loss: 1.0161021947860718 epoch 4 batch 40 \n",
      "total loss: 1.1519711017608643 epoch 4 batch 45 \n",
      "total loss: 1.1817655563354492 epoch 4 batch 50 \n",
      "total loss: 1.1651136875152588 epoch 4 batch 55 \n",
      "total loss: 1.1446980237960815 epoch 4 batch 60 \n",
      "total loss: 1.117824912071228 epoch 4 batch 65 \n",
      "total loss: 1.176274061203003 epoch 4 batch 70 \n",
      "total loss: 1.237805962562561 epoch 4 batch 75 \n",
      "total loss: 1.1074392795562744 epoch 4 batch 80 \n",
      "total loss: 1.1807115077972412 epoch 4 batch 85 \n",
      "total loss: 1.1747877597808838 epoch 4 batch 90 \n",
      "total loss: 1.1959308385849 epoch 4 batch 95 \n",
      "total loss: 1.125176191329956 epoch 4 batch 100 \n",
      "total loss: 1.16763174533844 epoch 4 batch 105 \n",
      "total loss: 1.0285274982452393 epoch 4 batch 110 \n",
      "total loss: 1.257751226425171 epoch 4 batch 115 \n",
      "total loss: 1.1241344213485718 epoch 4 batch 120 \n",
      "total loss: 1.171901822090149 epoch 4 batch 125 \n",
      "total loss: 1.0344364643096924 epoch 5 batch 5 \n",
      "total loss: 0.8743257522583008 epoch 5 batch 10 \n",
      "total loss: 1.0481934547424316 epoch 5 batch 15 \n",
      "total loss: 0.9659061431884766 epoch 5 batch 20 \n",
      "total loss: 0.9362924695014954 epoch 5 batch 25 \n",
      "total loss: 0.9079828262329102 epoch 5 batch 30 \n",
      "total loss: 0.9615944623947144 epoch 5 batch 35 \n",
      "total loss: 1.082787036895752 epoch 5 batch 40 \n",
      "total loss: 0.9623649716377258 epoch 5 batch 45 \n",
      "total loss: 1.0066635608673096 epoch 5 batch 50 \n",
      "total loss: 0.9191907644271851 epoch 5 batch 55 \n",
      "total loss: 0.9589571952819824 epoch 5 batch 60 \n",
      "total loss: 0.9199355840682983 epoch 5 batch 65 \n",
      "total loss: 0.898826003074646 epoch 5 batch 70 \n",
      "total loss: 0.9063251614570618 epoch 5 batch 75 \n",
      "total loss: 0.9836173057556152 epoch 5 batch 80 \n",
      "total loss: 0.9706042408943176 epoch 5 batch 85 \n",
      "total loss: 1.1125373840332031 epoch 5 batch 90 \n",
      "total loss: 0.9577423930168152 epoch 5 batch 95 \n",
      "total loss: 1.007083535194397 epoch 5 batch 100 \n",
      "total loss: 0.8678947687149048 epoch 5 batch 105 \n",
      "total loss: 1.0144853591918945 epoch 5 batch 110 \n",
      "total loss: 1.0609142780303955 epoch 5 batch 115 \n",
      "total loss: 1.0026209354400635 epoch 5 batch 120 \n",
      "total loss: 1.003309965133667 epoch 5 batch 125 \n",
      "total loss: 0.8267113566398621 epoch 6 batch 5 \n",
      "total loss: 0.8979266881942749 epoch 6 batch 10 \n",
      "total loss: 0.7488522529602051 epoch 6 batch 15 \n",
      "total loss: 0.7940376996994019 epoch 6 batch 20 \n",
      "total loss: 0.7443287968635559 epoch 6 batch 25 \n",
      "total loss: 0.8056058883666992 epoch 6 batch 30 \n",
      "total loss: 0.8314321637153625 epoch 6 batch 35 \n",
      "total loss: 0.9362083673477173 epoch 6 batch 40 \n",
      "total loss: 0.7870255708694458 epoch 6 batch 45 \n",
      "total loss: 0.9076045155525208 epoch 6 batch 50 \n",
      "total loss: 0.7920793294906616 epoch 6 batch 55 \n",
      "total loss: 0.8253324627876282 epoch 6 batch 60 \n",
      "total loss: 0.8761822581291199 epoch 6 batch 65 \n",
      "total loss: 0.8824746012687683 epoch 6 batch 70 \n",
      "total loss: 0.8608942031860352 epoch 6 batch 75 \n",
      "total loss: 0.8724573254585266 epoch 6 batch 80 \n",
      "total loss: 0.8794227838516235 epoch 6 batch 85 \n",
      "total loss: 0.7655300498008728 epoch 6 batch 90 \n",
      "total loss: 0.8919740915298462 epoch 6 batch 95 \n",
      "total loss: 0.8703159093856812 epoch 6 batch 100 \n",
      "total loss: 0.7860242128372192 epoch 6 batch 105 \n",
      "total loss: 0.9614154696464539 epoch 6 batch 110 \n",
      "total loss: 0.8240971565246582 epoch 6 batch 115 \n",
      "total loss: 0.8278889656066895 epoch 6 batch 120 \n",
      "total loss: 0.9316240549087524 epoch 6 batch 125 \n",
      "total loss: 0.6087136268615723 epoch 7 batch 5 \n",
      "total loss: 0.6677912473678589 epoch 7 batch 10 \n",
      "total loss: 0.6025391817092896 epoch 7 batch 15 \n",
      "total loss: 0.7053536176681519 epoch 7 batch 20 \n",
      "total loss: 0.6489280462265015 epoch 7 batch 25 \n",
      "total loss: 0.6406756639480591 epoch 7 batch 30 \n",
      "total loss: 0.788566529750824 epoch 7 batch 35 \n",
      "total loss: 0.6904456615447998 epoch 7 batch 40 \n",
      "total loss: 0.7217317819595337 epoch 7 batch 45 \n",
      "total loss: 0.6914864778518677 epoch 7 batch 50 \n",
      "total loss: 0.6596733331680298 epoch 7 batch 55 \n",
      "total loss: 0.6518681645393372 epoch 7 batch 60 \n",
      "total loss: 0.7601415514945984 epoch 7 batch 65 \n",
      "total loss: 0.7691682577133179 epoch 7 batch 70 \n",
      "total loss: 0.7889726758003235 epoch 7 batch 75 \n",
      "total loss: 0.6902936697006226 epoch 7 batch 80 \n",
      "total loss: 0.5923823118209839 epoch 7 batch 85 \n",
      "total loss: 0.662708580493927 epoch 7 batch 90 \n",
      "total loss: 0.708175539970398 epoch 7 batch 95 \n",
      "total loss: 0.7138915657997131 epoch 7 batch 100 \n",
      "total loss: 0.7520013451576233 epoch 7 batch 105 \n",
      "total loss: 0.7710679173469543 epoch 7 batch 110 \n",
      "total loss: 0.716265857219696 epoch 7 batch 115 \n",
      "total loss: 0.802302360534668 epoch 7 batch 120 \n",
      "total loss: 0.7361789345741272 epoch 7 batch 125 \n",
      "total loss: 0.6001310348510742 epoch 8 batch 5 \n",
      "total loss: 0.5611270666122437 epoch 8 batch 10 \n",
      "total loss: 0.5494142174720764 epoch 8 batch 15 \n",
      "total loss: 0.5461664199829102 epoch 8 batch 20 \n",
      "total loss: 0.5547149777412415 epoch 8 batch 25 \n",
      "total loss: 0.5099228620529175 epoch 8 batch 30 \n",
      "total loss: 0.6564574241638184 epoch 8 batch 35 \n",
      "total loss: 0.5976120829582214 epoch 8 batch 40 \n",
      "total loss: 0.5980761647224426 epoch 8 batch 45 \n",
      "total loss: 0.561428427696228 epoch 8 batch 50 \n",
      "total loss: 0.6032789945602417 epoch 8 batch 55 \n",
      "total loss: 0.6520915031433105 epoch 8 batch 60 \n",
      "total loss: 0.6568672060966492 epoch 8 batch 65 \n",
      "total loss: 0.5812273621559143 epoch 8 batch 70 \n",
      "total loss: 0.673845112323761 epoch 8 batch 75 \n",
      "total loss: 0.6158984899520874 epoch 8 batch 80 \n",
      "total loss: 0.6420877575874329 epoch 8 batch 85 \n",
      "total loss: 0.591168999671936 epoch 8 batch 90 \n",
      "total loss: 0.6025053262710571 epoch 8 batch 95 \n",
      "total loss: 0.545141875743866 epoch 8 batch 100 \n",
      "total loss: 0.6163442730903625 epoch 8 batch 105 \n",
      "total loss: 0.6014139652252197 epoch 8 batch 110 \n",
      "total loss: 0.5620930790901184 epoch 8 batch 115 \n",
      "total loss: 0.6324479579925537 epoch 8 batch 120 \n",
      "total loss: 0.5701943635940552 epoch 8 batch 125 \n",
      "total loss: 0.48939046263694763 epoch 9 batch 5 \n",
      "total loss: 0.4672769010066986 epoch 9 batch 10 \n",
      "total loss: 0.4356602728366852 epoch 9 batch 15 \n",
      "total loss: 0.466993510723114 epoch 9 batch 20 \n",
      "total loss: 0.5343005657196045 epoch 9 batch 25 \n",
      "total loss: 0.480719655752182 epoch 9 batch 30 \n",
      "total loss: 0.47377365827560425 epoch 9 batch 35 \n",
      "total loss: 0.4620538651943207 epoch 9 batch 40 \n",
      "total loss: 0.524666965007782 epoch 9 batch 45 \n",
      "total loss: 0.5804541707038879 epoch 9 batch 50 \n",
      "total loss: 0.443803071975708 epoch 9 batch 55 \n",
      "total loss: 0.5575132369995117 epoch 9 batch 60 \n",
      "total loss: 0.5475791692733765 epoch 9 batch 65 \n",
      "total loss: 0.5117989778518677 epoch 9 batch 70 \n",
      "total loss: 0.5096583366394043 epoch 9 batch 75 \n",
      "total loss: 0.464586079120636 epoch 9 batch 80 \n",
      "total loss: 0.5177216529846191 epoch 9 batch 85 \n",
      "total loss: 0.5378409624099731 epoch 9 batch 90 \n",
      "total loss: 0.5026952028274536 epoch 9 batch 95 \n",
      "total loss: 0.4998449385166168 epoch 9 batch 100 \n",
      "total loss: 0.4520260691642761 epoch 9 batch 105 \n",
      "total loss: 0.5125259160995483 epoch 9 batch 110 \n",
      "total loss: 0.46388429403305054 epoch 9 batch 115 \n",
      "total loss: 0.5618097186088562 epoch 9 batch 120 \n",
      "total loss: 0.5469890832901001 epoch 9 batch 125 \n",
      "total loss: 0.38065305352211 epoch 10 batch 5 \n",
      "total loss: 0.3531638979911804 epoch 10 batch 10 \n",
      "total loss: 0.3611612319946289 epoch 10 batch 15 \n",
      "total loss: 0.4034584164619446 epoch 10 batch 20 \n",
      "total loss: 0.3458257019519806 epoch 10 batch 25 \n",
      "total loss: 0.3856622278690338 epoch 10 batch 30 \n",
      "total loss: 0.38345661759376526 epoch 10 batch 35 \n",
      "total loss: 0.4088957905769348 epoch 10 batch 40 \n",
      "total loss: 0.43906545639038086 epoch 10 batch 45 \n",
      "total loss: 0.39241260290145874 epoch 10 batch 50 \n",
      "total loss: 0.4431312680244446 epoch 10 batch 55 \n",
      "total loss: 0.4188222289085388 epoch 10 batch 60 \n",
      "total loss: 0.39901602268218994 epoch 10 batch 65 \n",
      "total loss: 0.3961849808692932 epoch 10 batch 70 \n",
      "total loss: 0.4128665328025818 epoch 10 batch 75 \n",
      "total loss: 0.41230568289756775 epoch 10 batch 80 \n",
      "total loss: 0.4467870593070984 epoch 10 batch 85 \n",
      "total loss: 0.37596774101257324 epoch 10 batch 90 \n",
      "total loss: 0.431210994720459 epoch 10 batch 95 \n",
      "total loss: 0.4372327923774719 epoch 10 batch 100 \n",
      "total loss: 0.4107121527194977 epoch 10 batch 105 \n",
      "total loss: 0.4712144434452057 epoch 10 batch 110 \n",
      "total loss: 0.48455697298049927 epoch 10 batch 115 \n",
      "total loss: 0.46523723006248474 epoch 10 batch 120 \n",
      "total loss: 0.4090043902397156 epoch 10 batch 125 \n",
      "total loss: 0.2994024157524109 epoch 11 batch 5 \n",
      "total loss: 0.33085063099861145 epoch 11 batch 10 \n",
      "total loss: 0.33325108885765076 epoch 11 batch 15 \n",
      "total loss: 0.35643523931503296 epoch 11 batch 20 \n",
      "total loss: 0.2523137927055359 epoch 11 batch 25 \n",
      "total loss: 0.3232167959213257 epoch 11 batch 30 \n",
      "total loss: 0.27093467116355896 epoch 11 batch 35 \n",
      "total loss: 0.3349185883998871 epoch 11 batch 40 \n",
      "total loss: 0.33726224303245544 epoch 11 batch 45 \n",
      "total loss: 0.28662097454071045 epoch 11 batch 50 \n",
      "total loss: 0.34888342022895813 epoch 11 batch 55 \n",
      "total loss: 0.34733396768569946 epoch 11 batch 60 \n",
      "total loss: 0.39102602005004883 epoch 11 batch 65 \n",
      "total loss: 0.39779824018478394 epoch 11 batch 70 \n",
      "total loss: 0.34253302216529846 epoch 11 batch 75 \n",
      "total loss: 0.33000221848487854 epoch 11 batch 80 \n",
      "total loss: 0.371776282787323 epoch 11 batch 85 \n",
      "total loss: 0.37171778082847595 epoch 11 batch 90 \n",
      "total loss: 0.3950039744377136 epoch 11 batch 95 \n",
      "total loss: 0.42743951082229614 epoch 11 batch 100 \n",
      "total loss: 0.38636770844459534 epoch 11 batch 105 \n",
      "total loss: 0.4057762026786804 epoch 11 batch 110 \n",
      "total loss: 0.4322377145290375 epoch 11 batch 115 \n",
      "total loss: 0.33115702867507935 epoch 11 batch 120 \n",
      "total loss: 0.37880077958106995 epoch 11 batch 125 \n",
      "total loss: 0.2513149380683899 epoch 12 batch 5 \n",
      "total loss: 0.24484042823314667 epoch 12 batch 10 \n",
      "total loss: 0.2366766482591629 epoch 12 batch 15 \n",
      "total loss: 0.2835787236690521 epoch 12 batch 20 \n",
      "total loss: 0.29862719774246216 epoch 12 batch 25 \n",
      "total loss: 0.29603418707847595 epoch 12 batch 30 \n",
      "total loss: 0.29326194524765015 epoch 12 batch 35 \n",
      "total loss: 0.23979589343070984 epoch 12 batch 40 \n",
      "total loss: 0.272733211517334 epoch 12 batch 45 \n",
      "total loss: 0.2837478816509247 epoch 12 batch 50 \n",
      "total loss: 0.28158554434776306 epoch 12 batch 55 \n",
      "total loss: 0.3325793743133545 epoch 12 batch 60 \n",
      "total loss: 0.25985246896743774 epoch 12 batch 65 \n",
      "total loss: 0.30662423372268677 epoch 12 batch 70 \n",
      "total loss: 0.2943229079246521 epoch 12 batch 75 \n",
      "total loss: 0.2041066586971283 epoch 12 batch 80 \n",
      "total loss: 0.3036150336265564 epoch 12 batch 85 \n",
      "total loss: 0.3046325743198395 epoch 12 batch 90 \n",
      "total loss: 0.3165470063686371 epoch 12 batch 95 \n",
      "total loss: 0.31441718339920044 epoch 12 batch 100 \n",
      "total loss: 0.34076356887817383 epoch 12 batch 105 \n",
      "total loss: 0.28933200240135193 epoch 12 batch 110 \n",
      "total loss: 0.27125170826911926 epoch 12 batch 115 \n",
      "total loss: 0.3558545708656311 epoch 12 batch 120 \n",
      "total loss: 0.30833908915519714 epoch 12 batch 125 \n",
      "total loss: 0.20399312674999237 epoch 13 batch 5 \n",
      "total loss: 0.21297068893909454 epoch 13 batch 10 \n",
      "total loss: 0.21182675659656525 epoch 13 batch 15 \n",
      "total loss: 0.20512227714061737 epoch 13 batch 20 \n",
      "total loss: 0.22255277633666992 epoch 13 batch 25 \n",
      "total loss: 0.2743104100227356 epoch 13 batch 30 \n",
      "total loss: 0.20744188129901886 epoch 13 batch 35 \n",
      "total loss: 0.26004093885421753 epoch 13 batch 40 \n",
      "total loss: 0.2528962194919586 epoch 13 batch 45 \n",
      "total loss: 0.311292439699173 epoch 13 batch 50 \n",
      "total loss: 0.2567053437232971 epoch 13 batch 55 \n",
      "total loss: 0.23955731093883514 epoch 13 batch 60 \n",
      "total loss: 0.1943850964307785 epoch 13 batch 65 \n",
      "total loss: 0.2694360613822937 epoch 13 batch 70 \n",
      "total loss: 0.33331456780433655 epoch 13 batch 75 \n",
      "total loss: 0.27295932173728943 epoch 13 batch 80 \n",
      "total loss: 0.27161210775375366 epoch 13 batch 85 \n",
      "total loss: 0.26083841919898987 epoch 13 batch 90 \n",
      "total loss: 0.27963128685951233 epoch 13 batch 95 \n",
      "total loss: 0.2810189723968506 epoch 13 batch 100 \n",
      "total loss: 0.22681736946105957 epoch 13 batch 105 \n",
      "total loss: 0.307327538728714 epoch 13 batch 110 \n",
      "total loss: 0.28854650259017944 epoch 13 batch 115 \n",
      "total loss: 0.35725265741348267 epoch 13 batch 120 \n",
      "total loss: 0.24709871411323547 epoch 13 batch 125 \n",
      "total loss: 0.17893747985363007 epoch 14 batch 5 \n",
      "total loss: 0.19061987102031708 epoch 14 batch 10 \n",
      "total loss: 0.18538089096546173 epoch 14 batch 15 \n",
      "total loss: 0.1791307032108307 epoch 14 batch 20 \n",
      "total loss: 0.16830766201019287 epoch 14 batch 25 \n",
      "total loss: 0.21465718746185303 epoch 14 batch 30 \n",
      "total loss: 0.15826988220214844 epoch 14 batch 35 \n",
      "total loss: 0.26830199360847473 epoch 14 batch 40 \n",
      "total loss: 0.17505498230457306 epoch 14 batch 45 \n",
      "total loss: 0.21376681327819824 epoch 14 batch 50 \n",
      "total loss: 0.23063364624977112 epoch 14 batch 55 \n",
      "total loss: 0.16646143794059753 epoch 14 batch 60 \n",
      "total loss: 0.24730773270130157 epoch 14 batch 65 \n",
      "total loss: 0.2139851152896881 epoch 14 batch 70 \n",
      "total loss: 0.23193106055259705 epoch 14 batch 75 \n",
      "total loss: 0.25384047627449036 epoch 14 batch 80 \n",
      "total loss: 0.2266257256269455 epoch 14 batch 85 \n",
      "total loss: 0.2659565806388855 epoch 14 batch 90 \n",
      "total loss: 0.20141470432281494 epoch 14 batch 95 \n",
      "total loss: 0.2682774066925049 epoch 14 batch 100 \n",
      "total loss: 0.24343009293079376 epoch 14 batch 105 \n",
      "total loss: 0.20636145770549774 epoch 14 batch 110 \n",
      "total loss: 0.23876599967479706 epoch 14 batch 115 \n",
      "total loss: 0.218269944190979 epoch 14 batch 120 \n",
      "total loss: 0.25959068536758423 epoch 14 batch 125 \n",
      "total loss: 0.19787879288196564 epoch 15 batch 5 \n",
      "total loss: 0.16941027343273163 epoch 15 batch 10 \n",
      "total loss: 0.17062458395957947 epoch 15 batch 15 \n",
      "total loss: 0.1624196171760559 epoch 15 batch 20 \n",
      "total loss: 0.1858682632446289 epoch 15 batch 25 \n",
      "total loss: 0.20647267997264862 epoch 15 batch 30 \n",
      "total loss: 0.1961928904056549 epoch 15 batch 35 \n",
      "total loss: 0.17021393775939941 epoch 15 batch 40 \n",
      "total loss: 0.2400718629360199 epoch 15 batch 45 \n",
      "total loss: 0.18626658618450165 epoch 15 batch 50 \n",
      "total loss: 0.20109209418296814 epoch 15 batch 55 \n",
      "total loss: 0.2317298948764801 epoch 15 batch 60 \n",
      "total loss: 0.19223730266094208 epoch 15 batch 65 \n",
      "total loss: 0.2181943655014038 epoch 15 batch 70 \n",
      "total loss: 0.25252437591552734 epoch 15 batch 75 \n",
      "total loss: 0.19796761870384216 epoch 15 batch 80 \n",
      "total loss: 0.19541709125041962 epoch 15 batch 85 \n",
      "total loss: 0.23190370202064514 epoch 15 batch 90 \n",
      "total loss: 0.21728713810443878 epoch 15 batch 95 \n",
      "total loss: 0.23605474829673767 epoch 15 batch 100 \n",
      "total loss: 0.2652929127216339 epoch 15 batch 105 \n",
      "total loss: 0.19496330618858337 epoch 15 batch 110 \n",
      "total loss: 0.2332514226436615 epoch 15 batch 115 \n",
      "total loss: 0.21730241179466248 epoch 15 batch 120 \n",
      "total loss: 0.23493751883506775 epoch 15 batch 125 \n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "for i in range(1, epochs+1):\n",
    "\n",
    "    encoder_initial_cell_state = initialize_initial_state()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
    "        total_loss += batch_loss\n",
    "        if (batch+1)%5 == 0:\n",
    "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
    "#through the length of the model, for this we use greedsampler to run through the decoder\n",
    "#and the final embedding matrix trained on the data is used to generate embeddings\n",
    "input_raw='how are you'\n",
    "\n",
    "# We have a transcript file containing English-German pairs\n",
    "# Preprocess X\n",
    "input_lines = ['<start> '+input_raw+'']\n",
    "input_sequences = [[en_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
    "                                                                maxlen=Tx, padding='post')\n",
    "inp = tf.convert_to_tensor(input_sequences)\n",
    "#print(inp.shape)\n",
    "inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
    "                              tf.zeros((inference_batch_size, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
    "                                                initial_state =encoder_initial_cell_state)\n",
    "print('a_tx :',a_tx.shape)\n",
    "print('c_tx :', c_tx.shape)\n",
    "\n",
    "start_tokens = tf.fill([inference_batch_size],ge_tokenizer.word_index['<start>'])\n",
    "\n",
    "end_token = ge_tokenizer.word_index['<end>']\n",
    "\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "decoder_input = tf.expand_dims([ge_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoderNetwork.dense_layer)\n",
    "decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array([a_tx, c_tx]).shape)\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
    "                                                                   encoder_state=[a_tx, c_tx],\n",
    "                                                                   Dtype=tf.float32)\n",
    "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
    " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
    "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
    "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
    "\n",
    "#initialize inference decoder\n",
    "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
    "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state  \n",
    "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
    "for j in range(maximum_iterations):\n",
    "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction based on our sentence earlier\n",
    "print(\"English Sentence:\")\n",
    "print(input_raw)\n",
    "print(\"\\nGerman Translation:\")\n",
    "for i in range(len(predictions)):\n",
    "    line = predictions[i,:]\n",
    "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
    "    print(\" \".join( [ge_tokenizer.index_word[w] for w in seq]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
