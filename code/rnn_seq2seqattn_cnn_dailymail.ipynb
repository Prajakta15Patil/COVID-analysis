{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Flatten, Concatenate, TimeDistributed, Bidirectional, Attention, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import TensorShape\n",
    "import tensorflow_addons as tfa\n",
    "from langdetect import detect\n",
    "import tensorflow_datasets as tfds\n",
    "import itertools\n",
    "from numba import jit, cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if not device_name:\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: prajakta15patil (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.11<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">dainty-glade-10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/prajakta15patil/COVID-analysis-code\" target=\"_blank\">https://wandb.ai/prajakta15patil/COVID-analysis-code</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/prajakta15patil/COVID-analysis-code/runs/4lfujqbv\" target=\"_blank\">https://wandb.ai/prajakta15patil/COVID-analysis-code/runs/4lfujqbv</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/COVID-analysis/code/wandb/run-20201128_205715-4lfujqbv</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(4lfujqbv)</h1><p></p><iframe src=\"https://wandb.ai/prajakta15patil/COVID-analysis-code/runs/4lfujqbv\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f700320b090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 15.4 s, total: 2min 46s\n",
      "Wall time: 1min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ds = tfds.load('cnn_dailymail', split='train', as_supervised=True)\n",
    "h,a = [],[]\n",
    "\n",
    "for article, highlights in ds:  \n",
    "    h.append(str(highlights.numpy()))  \n",
    "    a.append(str(article.numpy()))\n",
    "cnn = pd.DataFrame(list(zip(a, h)), \n",
    "               columns =['article', 'highlights']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.1 s, sys: 9.57 s, total: 1min\n",
      "Wall time: 1min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>len_bt</th>\n",
       "      <th>len_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"By . Associated Press . PUBLISHED: . 14:11 E...</td>\n",
       "      <td>b'Bishop John Folda, of North Dakota, is takin...</td>\n",
       "      <td>198</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'(CNN) -- Ralph Mata was an internal affairs ...</td>\n",
       "      <td>b'Criminal complaint: Cop used his role to hel...</td>\n",
       "      <td>392</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b\"A drunk driver who killed a young woman in a...</td>\n",
       "      <td>b\"Craig Eccleston-Todd, 27, had drunk at least...</td>\n",
       "      <td>800</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b\"(CNN) -- With a breezy sweep of his pen Pres...</td>\n",
       "      <td>b\"Nina dos Santos says Europe must be ready to...</td>\n",
       "      <td>531</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"Fleetwood are the only team still to have a ...</td>\n",
       "      <td>b'Fleetwood top of League One after 2-0 win at...</td>\n",
       "      <td>580</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>b\"Firemen in Germany were forced to don gas ma...</td>\n",
       "      <td>b\"Conditions inside the flat in Nurtigen, Germ...</td>\n",
       "      <td>499</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>b'The 298 victims on board tragic flight MH17 ...</td>\n",
       "      <td>b\"Expert says MH17 passengers would likely 'no...</td>\n",
       "      <td>800</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>b\"By . Charlie Skillen . Follow @@charlieskill...</td>\n",
       "      <td>b\"Diego Costa took part in Atletico Madrid's f...</td>\n",
       "      <td>391</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>b\"By . Martin Robinson . PUBLISHED: . 03:55 ES...</td>\n",
       "      <td>b\"Matt Pullen was due to become Managing Direc...</td>\n",
       "      <td>519</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>b'By . Nick Harris . What price a World Cup? T...</td>\n",
       "      <td>b'FIFA prepare to earn at least \\xc2\\xa32.55bi...</td>\n",
       "      <td>502</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                article  \\\n",
       "0     b\"By . Associated Press . PUBLISHED: . 14:11 E...   \n",
       "1     b'(CNN) -- Ralph Mata was an internal affairs ...   \n",
       "2     b\"A drunk driver who killed a young woman in a...   \n",
       "3     b\"(CNN) -- With a breezy sweep of his pen Pres...   \n",
       "4     b\"Fleetwood are the only team still to have a ...   \n",
       "...                                                 ...   \n",
       "1995  b\"Firemen in Germany were forced to don gas ma...   \n",
       "1996  b'The 298 victims on board tragic flight MH17 ...   \n",
       "1997  b\"By . Charlie Skillen . Follow @@charlieskill...   \n",
       "1998  b\"By . Martin Robinson . PUBLISHED: . 03:55 ES...   \n",
       "1999  b'By . Nick Harris . What price a World Cup? T...   \n",
       "\n",
       "                                             highlights  len_bt  len_ab  \n",
       "0     b'Bishop John Folda, of North Dakota, is takin...     198      35  \n",
       "1     b'Criminal complaint: Cop used his role to hel...     392      36  \n",
       "2     b\"Craig Eccleston-Todd, 27, had drunk at least...     800      65  \n",
       "3     b\"Nina dos Santos says Europe must be ready to...     531      51  \n",
       "4     b'Fleetwood top of League One after 2-0 win at...     580      62  \n",
       "...                                                 ...     ...     ...  \n",
       "1995  b\"Conditions inside the flat in Nurtigen, Germ...     499      51  \n",
       "1996  b\"Expert says MH17 passengers would likely 'no...     800      71  \n",
       "1997  b\"Diego Costa took part in Atletico Madrid's f...     391      29  \n",
       "1998  b\"Matt Pullen was due to become Managing Direc...     519      51  \n",
       "1999  b'FIFA prepare to earn at least \\xc2\\xa32.55bi...     502      34  \n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn['article'] = cnn.article.apply(lambda x : x.split()[:800] if len(x.split())>800 else x.split())\n",
    "cnn['highlights'] = cnn.highlights.apply(lambda x : x.split()[:800] if len(x.split())>800 else x.split())\n",
    "\n",
    "cnn['article'] = cnn.article.apply(lambda x: (\" \").join(x))\n",
    "cnn['highlights'] = cnn.highlights.apply(lambda x: (\" \").join(x))\n",
    "\n",
    "cnn['len_bt'] = cnn.article.map(lambda x: len(x.split(\" \")))\n",
    "cnn['len_ab'] = cnn.highlights.map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#cnn.query('len_bt <= 1000 and len_ab <= 30', inplace = True)\n",
    "cnn = cnn[:2000]\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f6ff85b69d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cnn.len_bt,cnn.len_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "path = '/home/ubuntu/data/*.json'\n",
    "files = glob.glob(path)\n",
    "papers = []\n",
    "for file in files:\n",
    "    with open(file) as json_file:\n",
    "            text = json.load(json_file)\n",
    "            papers.append([text['paper_id'], text['bodytext'], text['abstract']])\n",
    "data = pd.DataFrame(papers, columns = ['paper_id', 'bodytext', 'abstract'])\n",
    "\n",
    "#get the lengths of texts\n",
    "data['len_bt'] = data.bodytext.map(lambda x: len(x.split(\" \")))\n",
    "data['len_ab'] = data.abstract.map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#filter papers with certain word length\n",
    "data.query('len_bt <= 10000 and len_bt>=100 and len_ab <= 500 and len_ab >=20', inplace = True)\n",
    "\n",
    "#detect languages of texts to filter out non-english papers\n",
    "data['bt_lang'] = data.bodytext.map(lambda x: detect(x))\n",
    "data['ab_lang'] = data.abstract.map(lambda x: detect(x))\n",
    "\n",
    "#use only english papers\n",
    "data = data[(data['bt_lang'] == 'en') & (data['ab_lang'] == 'en')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Reduce vocab_size\n",
    "# start with smaller sample. 1000/2000\n",
    "# truncate the papers to certain word size\n",
    "# remove words with a certain frequency. start with 2000 words. \n",
    "# remove words from bodytext and feed it to the model. \n",
    "\n",
    "#run the function API code on https://www.tensorflow.org/datasets/catalog/cnn_dailymail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(data.len_bt,data.len_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit(target =\"cuda\")\n",
    "def clean_text(bodytext):\n",
    "    cleaned = list()\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for word in bodytext:\n",
    "        words = str(word)       \n",
    "        words = words.lower()\n",
    "        words = words.translate(table)\n",
    "        words = re_print.sub('', words) \n",
    "        if words.isalpha() == True:\n",
    "            cleaned.append(words)\n",
    "    cleaned.insert(0, '<start>')\n",
    "    cleaned.append('<end>')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "bt_vector = list()\n",
    "bt_list = []\n",
    "ab_list = []\n",
    "for i in range(len(cnn)):\n",
    "    bodytext = nlp(cnn.iloc[i].article)\n",
    "    bt_clean = clean_text(bodytext)\n",
    "    bt_list.append(bt_clean)\n",
    "    \n",
    "    abstract = nlp(cnn.iloc[i].highlights)\n",
    "    ab_clean = clean_text(abstract)\n",
    "    ab_list.append(ab_clean)\n",
    "com_list = ab_list + bt_list\n",
    "    #c_papers.append(papers)\n",
    "bt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "bt_tokenizer.fit_on_texts(com_list)\n",
    "data_bt = bt_tokenizer.texts_to_sequences(bt_list)\n",
    "data_ab = bt_tokenizer.texts_to_sequences(ab_list)\n",
    "\n",
    "longest_seq = max(max([len(x) for x in data_bt]), max([len(x) for x in data_ab]))\n",
    "#x_voc_size = max([len(x) for x in data_bt])#, max([len(x) for x in data_ab]))\n",
    "#y_voc_size = max([len(y) for y in data_ab])\n",
    "data_bt = tf.keras.preprocessing.sequence.pad_sequences(data_bt,padding='post', maxlen = longest_seq)\n",
    "data_ab = tf.keras.preprocessing.sequence.pad_sequences(data_ab,padding='post', maxlen = longest_seq) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timecheck cuda version\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "bt_vector = list()\n",
    "bt_list = []\n",
    "ab_list = []\n",
    "for i in range(len(data)):\n",
    "    bodytext = nlp(data.iloc[i].bodytext)\n",
    "    bt_clean = clean_text(bodytext)\n",
    "    bt_list.append(bt_clean)\n",
    "    \n",
    "    abstract = nlp(data.iloc[i].abstract)\n",
    "    ab_clean = clean_text(abstract)\n",
    "    ab_list.append(ab_clean)\n",
    "com_list = ab_list + bt_list\n",
    "    #c_papers.append(papers)\n",
    "bt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "bt_tokenizer.fit_on_texts(com_list)\n",
    "data_bt = bt_tokenizer.texts_to_sequences(bt_list)\n",
    "data_ab = bt_tokenizer.texts_to_sequences(ab_list)\n",
    "\n",
    "longest_seq = max(max([len(x) for x in data_bt]), max([len(x) for x in data_ab]))\n",
    "#x_voc_size = max([len(x) for x in data_bt])#, max([len(x) for x in data_ab]))\n",
    "#y_voc_size = max([len(y) for y in data_ab])\n",
    "data_bt = tf.keras.preprocessing.sequence.pad_sequences(data_bt,padding='post', maxlen = longest_seq)\n",
    "data_ab = tf.keras.preprocessing.sequence.pad_sequences(data_ab,padding='post', maxlen = longest_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(tensor):\n",
    "    #print( np.argmax([len(t) for t in tensor]))\n",
    "    return max( len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(data_bt,data_ab,test_size=0.2)\n",
    "BATCH_SIZE = 20\n",
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 256\n",
    "rnn_units = 64\n",
    "dense_units = 64\n",
    "Dtype = tf.float32\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = bt_tokenizer.word_index\n",
    "print(list(a.keys())[list(a.values()).index(9326)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(bt_tokenizer.word_index)+1  \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_initial_state():\n",
    "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n",
    "encoder_initial_cell_state = initialize_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "class EncoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
    "                                                           output_dim=embedding_dims)\n",
    "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
    "                                                     return_state=True )\n",
    "    \n",
    "#DECODER\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
    "        super().__init__()\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
    "                                                           output_dim=embedding_dims) \n",
    "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[longest_seq]) \n",
    "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
    "                                                output_layer=self.dense_layer)\n",
    "\n",
    "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
    "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
    "                                          memory_sequence_length=memory_sequence_length)\n",
    "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    # wrap decodernn cell  \n",
    "    def build_rnn_cell(self, batch_size ):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
    "                                                attention_layer_size=dense_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
    "                                                                dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "        return decoder_initial_state\n",
    "\n",
    "encoderNetwork = EncoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "decoderNetwork = DecoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.saved_model.save(encoderNetwork, \"./covid_encoder\")\n",
    "tf.saved_model.save(decoderNetwork, \"./covid_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y):\n",
    "   \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "\n",
    "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
    "        \n",
    "         \n",
    "        # Prepare correct Decoder input & output sequence data\n",
    "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "        #compare logits with timestepped +1 version of decoder_input\n",
    "        decoder_output = output_batch[:,1:] #ignore <start>\n",
    "\n",
    "\n",
    "        # Decoder Embeddings\n",
    "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "        \n",
    "        #BasicDecoderOutput        \n",
    "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                               sequence_length=BATCH_SIZE*[longest_seq-1])\n",
    "\n",
    "        logits = outputs.rnn_output\n",
    "        #Calculate loss\n",
    "\n",
    "        loss = loss_function(logits, decoder_output)\n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars – List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_train, Y_train, batch_size):\n",
    "    num_samples = len(X_train)\n",
    "    print(num_samples)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            yield X_train[offset:offset+batch_size], Y_train[offset:offset+batch_size]\n",
    "    \n",
    "train_samples = generator(X_train, Y_train, batch_size = 8000)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(next(train_samples)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ijk =0\n",
    "for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    ijk+=1\n",
    "print(ijk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(next(train_samples)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "epochs = 100\n",
    "for i in range(1, epochs+1):\n",
    "\n",
    "    encoder_initial_cell_state = initialize_initial_state()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "\n",
    "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
    "        total_loss += batch_loss\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(next(train_samples)).batch(BATCH_SIZE, drop_remainder=True)\n",
    "        #print(next(train_samples))\n",
    "        if (batch+1)%2 == 0:\n",
    "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
    "#through the length of the model, for this we use greedsampler to run through the decoder\n",
    "#and the final embedding matrix trained on the data is used to generate embeddings\n",
    "input_raw=X_test[0].reshape(1,-1)\n",
    "# inp_bodytext = nlp(input_raw)\n",
    "# input_lines = clean_text(inp_bodytext)\n",
    "\n",
    "# We have a transcript file containing English-German pairs\n",
    "# Preprocess X\n",
    "#input_raw = clean_text(input_raw)\n",
    "#input_lines = [f'{bt_tok} {input_raw}']\n",
    "# input_sequences = [[bt_tokenizer.word_index[w] for w in line.split()] for line in input_raw]\n",
    "# input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_raw,\n",
    "#                                                                maxlen=longest_seq, padding='post')\n",
    "# inp = tf.convert_to_tensor(input_sequences)\n",
    "\n",
    "#print(\"inp\", inp.shape)\n",
    "#print(\"inp_seq\",input_sequences)\n",
    "#inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((1, rnn_units)),tf.zeros((1, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(input_raw)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,initial_state =encoder_initial_cell_state)\n",
    "# print('a_tx :', a_tx.shape)\n",
    "# print('c_tx :', c_tx.shape)\n",
    "\n",
    "start_tokens = tf.fill([1],bt_tokenizer.word_index['<start>'])\n",
    "\n",
    "end_token = bt_tokenizer.word_index['<end>']\n",
    "\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "decoder_input = tf.expand_dims([bt_tokenizer.word_index['<start>']]* 1,1)\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoderNetwork.dense_layer)\n",
    "decoderNetwork.attention_mechanism.setup_memory(c_tx)\n",
    "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "# print(f\"decoder_initial_state = [a_tx, c_tx] : {np.array([a_tx, c_tx]).shape}\")\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(1,\n",
    "                                                                   encoder_state=[a_tx, c_tx],\n",
    "                                                                   Dtype=tf.float32)\n",
    "# print(f\"\"\"\n",
    "# Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
    "# is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
    "\n",
    "# decoder initial state shape: {np.array(decoder_initial_state).shape}\n",
    "# decoder_initial_state tensor\n",
    "# {decoder_initial_state}\n",
    "# \"\"\")\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(longest_seq)*2)\n",
    "\n",
    "#initialize inference decoder\n",
    "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "#print(f\"first_inputs returns the same decoder_input i.e. embedding of  {'<start>'} : {first_inputs.shape}\")\n",
    "#print(f\"start_index_emb_avg {tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))}\") # mean along the batch\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state \n",
    "\n",
    "predictions = np.empty((1,0), dtype = np.int32)                                                                             \n",
    "for j in range(maximum_iterations):\n",
    "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    #print(next_inputs)\n",
    "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSummary:\")\n",
    "for i in range(len(predictions)):\n",
    "    line = predictions[i,:]\n",
    "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
    "    print(\" \".join( [bt_tokenizer.index_word[w] for w in seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
