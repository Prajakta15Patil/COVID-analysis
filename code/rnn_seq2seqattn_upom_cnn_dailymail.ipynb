{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense,Flatten, Concatenate, TimeDistributed, Bidirectional, Attention, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import TensorShape\n",
    "import tensorflow_addons as tfa\n",
    "from langdetect import detect\n",
    "import tensorflow_datasets as tfds\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 12.1 s, total: 2min 29s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ds = tfds.load('cnn_dailymail', split='train', as_supervised=True)\n",
    "h,a = [],[]\n",
    "\n",
    "for article, highlights in ds:  \n",
    "    h.append(str(highlights.numpy()))  \n",
    "    a.append(str(article.numpy()))\n",
    "cnn = pd.DataFrame(list(zip(a, h)), \n",
    "               columns =['article', 'highlights']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feed data slowly to model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 s, sys: 0 ns, total: 20.2 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cnn.article = cnn.article.apply(lambda x : x.rsplit(maxsplit=len(x.split())-70)[0])\n",
    "cnn.highlights = cnn.highlights.apply(lambda x : x.rsplit(maxsplit=len(x.split())-50)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn['len_bt'] = cnn.article.map(lambda x: len(x.split(\" \")))\n",
    "cnn['len_ab'] = cnn.highlights.map(lambda x: len(x.split(\" \")))\n",
    "#cnn.query('len_bt <= 1000 and len_ab <= 30', inplace = True)\n",
    "cnn = cnn[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_bt</th>\n",
       "      <th>len_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>70.008200</td>\n",
       "      <td>20.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.715234</td>\n",
       "      <td>23.949265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             len_bt        len_ab\n",
       "count  10000.000000  10000.000000\n",
       "mean      70.008200     20.265300\n",
       "std        1.715234     23.949265\n",
       "min        1.000000      1.000000\n",
       "25%       70.000000      1.000000\n",
       "50%       70.000000      1.000000\n",
       "75%       70.000000     50.000000\n",
       "max       83.000000     53.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>len_bt</th>\n",
       "      <th>len_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b\"By . Associated Press . PUBLISHED: . 14:11 E...</td>\n",
       "      <td>b'Bishop</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'(CNN) -- Ralph Mata was an internal affairs ...</td>\n",
       "      <td>b'Criminal</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b\"A drunk driver who killed a young woman in a...</td>\n",
       "      <td>b\"Craig Eccleston-Todd, 27, had drunk at least...</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b\"(CNN) -- With a breezy sweep of his pen Pres...</td>\n",
       "      <td>b\"Nina dos Santos says Europe must be ready to...</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"Fleetwood are the only team still to have a ...</td>\n",
       "      <td>b'Fleetwood top of League One after 2-0 win at...</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  b\"By . Associated Press . PUBLISHED: . 14:11 E...   \n",
       "1  b'(CNN) -- Ralph Mata was an internal affairs ...   \n",
       "2  b\"A drunk driver who killed a young woman in a...   \n",
       "3  b\"(CNN) -- With a breezy sweep of his pen Pres...   \n",
       "4  b\"Fleetwood are the only team still to have a ...   \n",
       "\n",
       "                                          highlights  len_bt  len_ab  \n",
       "0                                           b'Bishop      70       1  \n",
       "1                                         b'Criminal      70       1  \n",
       "2  b\"Craig Eccleston-Todd, 27, had drunk at least...      70      50  \n",
       "3  b\"Nina dos Santos says Europe must be ready to...      70      50  \n",
       "4  b'Fleetwood top of League One after 2-0 win at...      70      50  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5246422ed0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(cnn.len_bt,cnn.len_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "path = '/home/ubuntu/data/*.json'\n",
    "files = glob.glob(path)\n",
    "papers = []\n",
    "for file in files:\n",
    "    with open(file) as json_file:\n",
    "            text = json.load(json_file)\n",
    "            papers.append([text['paper_id'], text['bodytext'], text['abstract']])\n",
    "data = pd.DataFrame(papers, columns = ['paper_id', 'bodytext', 'abstract'])\n",
    "\n",
    "#get the lengths of texts\n",
    "data['len_bt'] = data.bodytext.map(lambda x: len(x.split(\" \")))\n",
    "data['len_ab'] = data.abstract.map(lambda x: len(x.split(\" \")))\n",
    "\n",
    "#filter papers with certain word length\n",
    "data.query('len_bt <= 10000 and len_bt>=100 and len_ab <= 500 and len_ab >=20', inplace = True)\n",
    "\n",
    "#detect languages of texts to filter out non-english papers\n",
    "data['bt_lang'] = data.bodytext.map(lambda x: detect(x))\n",
    "data['ab_lang'] = data.abstract.map(lambda x: detect(x))\n",
    "\n",
    "#use only english papers\n",
    "data = data[(data['bt_lang'] == 'en') & (data['ab_lang'] == 'en')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Reduce vocab_size\n",
    "# start with smaller sample. 1000/2000\n",
    "# truncate the papers to certain word size\n",
    "# remove words with a certain frequency. start with 2000 words. \n",
    "# remove words from bodytext and feed it to the model. \n",
    "\n",
    "#run the function API code on https://www.tensorflow.org/datasets/catalog/cnn_dailymail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.scatter(data.len_bt,data.len_ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(bodytext):\n",
    "    cleaned = list()\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for word in bodytext:\n",
    "        words = str(word)       \n",
    "        words = words.lower()\n",
    "        words = words.translate(table)\n",
    "        words = re_print.sub('', words) \n",
    "        if words.isalpha() == True:\n",
    "            cleaned.append(words)\n",
    "    cleaned.insert(0, '<start>')\n",
    "    cleaned.append('<end>')\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 20s, sys: 0 ns, total: 5min 20s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "bt_vector = list()\n",
    "bt_list = []\n",
    "ab_list = []\n",
    "for i in range(len(cnn)):\n",
    "    bodytext = nlp(cnn.iloc[i].article)\n",
    "    bt_clean = clean_text(bodytext)\n",
    "    bt_list.append(bt_clean)\n",
    "    \n",
    "    abstract = nlp(cnn.iloc[i].highlights)\n",
    "    ab_clean = clean_text(abstract)\n",
    "    ab_list.append(ab_clean)\n",
    "com_list = ab_list + bt_list\n",
    "    #c_papers.append(papers)\n",
    "bt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "bt_tokenizer.fit_on_texts(com_list)\n",
    "data_bt = bt_tokenizer.texts_to_sequences(bt_list)\n",
    "data_ab = bt_tokenizer.texts_to_sequences(ab_list)\n",
    "\n",
    "longest_seq = max(max([len(x) for x in data_bt]), max([len(x) for x in data_ab]))\n",
    "#x_voc_size = max([len(x) for x in data_bt])#, max([len(x) for x in data_ab]))\n",
    "#y_voc_size = max([len(y) for y in data_ab])\n",
    "data_bt = tf.keras.preprocessing.sequence.pad_sequences(data_bt,padding='post', maxlen = longest_seq)\n",
    "data_ab = tf.keras.preprocessing.sequence.pad_sequences(data_ab,padding='post', maxlen = longest_seq) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "bt_vector = list()\n",
    "bt_list = []\n",
    "ab_list = []\n",
    "for i in range(len(data)):\n",
    "    bodytext = nlp(data.iloc[i].bodytext)\n",
    "    bt_clean = clean_text(bodytext)\n",
    "    bt_list.append(bt_clean)\n",
    "    \n",
    "    abstract = nlp(data.iloc[i].abstract)\n",
    "    ab_clean = clean_text(abstract)\n",
    "    ab_list.append(ab_clean)\n",
    "com_list = ab_list + bt_list\n",
    "    #c_papers.append(papers)\n",
    "bt_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "bt_tokenizer.fit_on_texts(com_list)\n",
    "data_bt = bt_tokenizer.texts_to_sequences(bt_list)\n",
    "data_ab = bt_tokenizer.texts_to_sequences(ab_list)\n",
    "\n",
    "longest_seq = max(max([len(x) for x in data_bt]), max([len(x) for x in data_ab]))\n",
    "#x_voc_size = max([len(x) for x in data_bt])#, max([len(x) for x in data_ab]))\n",
    "#y_voc_size = max([len(y) for y in data_ab])\n",
    "data_bt = tf.keras.preprocessing.sequence.pad_sequences(data_bt,padding='post', maxlen = longest_seq)\n",
    "data_ab = tf.keras.preprocessing.sequence.pad_sequences(data_ab,padding='post', maxlen = longest_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(tensor):\n",
    "    #print( np.argmax([len(t) for t in tensor]))\n",
    "    return max( len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(data_bt,data_ab,test_size=0.2)\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = len(X_train)\n",
    "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dims = 256\n",
    "rnn_units = 64\n",
    "dense_units = 64\n",
    "Dtype = tf.float32\n",
    "#dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = bt_tokenizer.word_index\n",
    "print(list(a.keys())[list(a.values()).index(9326)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46921"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(bt_tokenizer.word_index)+1  \n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_initial_state():\n",
    "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]\n",
    "encoder_initial_cell_state = initialize_initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER\n",
    "class EncoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
    "        super().__init__()\n",
    "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
    "                                                           output_dim=embedding_dims)\n",
    "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
    "                                                     return_state=True )\n",
    "    \n",
    "#DECODER\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
    "        super().__init__()\n",
    "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
    "                                                           output_dim=embedding_dims) \n",
    "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
    "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
    "        # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[longest_seq]) \n",
    "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
    "                                                output_layer=self.dense_layer)\n",
    "\n",
    "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
    "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
    "                                          memory_sequence_length=memory_sequence_length)\n",
    "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    # wrap decodernn cell  \n",
    "    def build_rnn_cell(self, batch_size ):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
    "                                                attention_layer_size=dense_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
    "                                                                dtype = Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
    "        return decoder_initial_state\n",
    "\n",
    "encoderNetwork = EncoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "decoderNetwork = DecoderNetwork(vocab_size,embedding_dims, rnn_units)\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf.saved_model.save(encoderNetwork, \"./covid_encoder\")\n",
    "tf.saved_model.save(decoderNetwork, \"./covid_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y):\n",
    "   \n",
    "    #shape of y [batch_size, ty]\n",
    "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
    "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                                                  reduction='none')\n",
    "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
    "    #initialize loss = 0\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
    "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
    "                                                        initial_state =encoder_initial_cell_state)\n",
    "\n",
    "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
    "        \n",
    "         \n",
    "        # Prepare correct Decoder input & output sequence data\n",
    "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
    "        #compare logits with timestepped +1 version of decoder_input\n",
    "        decoder_output = output_batch[:,1:] #ignore <start>\n",
    "\n",
    "\n",
    "        # Decoder Embeddings\n",
    "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
    "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
    "                                                                           encoder_state=[a_tx, c_tx],\n",
    "                                                                           Dtype=tf.float32)\n",
    "        \n",
    "        #BasicDecoderOutput        \n",
    "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
    "                                               sequence_length=BATCH_SIZE*[longest_seq-1])\n",
    "\n",
    "        logits = outputs.rnn_output\n",
    "        #Calculate loss\n",
    "\n",
    "        loss = loss_function(logits, decoder_output)\n",
    "\n",
    "    #Returns the list of all layer variables / weights.\n",
    "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
    "    # differentiate loss wrt variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    #grads_and_vars â€“ List of(gradient, variable) pairs.\n",
    "    grads_and_vars = zip(gradients,variables)\n",
    "    optimizer.apply_gradients(grads_and_vars)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_train, Y_train, batch_size):\n",
    "    num_samples = len(X_train)\n",
    "    print(num_samples)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            yield X_train[offset:offset+batch_size], Y_train[offset:offset+batch_size]\n",
    "    \n",
    "train_samples = generator(X_train, Y_train, batch_size = 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(next(train_samples)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loss: 3.4760000705718994 epoch 1 batch 2 \n",
      "total loss: 3.4754750728607178 epoch 1 batch 4 \n",
      "total loss: 3.409573554992676 epoch 1 batch 6 \n",
      "total loss: 0.261798620223999 epoch 2 batch 2 \n",
      "total loss: 0.26156753301620483 epoch 2 batch 4 \n",
      "total loss: 6.333439350128174 epoch 2 batch 6 \n",
      "total loss: 0.2604919970035553 epoch 3 batch 2 \n",
      "total loss: 3.482128620147705 epoch 3 batch 4 \n",
      "total loss: 3.3008999824523926 epoch 3 batch 6 \n",
      "total loss: 3.2071681022644043 epoch 4 batch 2 \n",
      "total loss: 3.1975204944610596 epoch 4 batch 4 \n",
      "total loss: 5.736441135406494 epoch 4 batch 6 \n",
      "total loss: 5.0985941886901855 epoch 5 batch 2 \n",
      "total loss: 2.490811347961426 epoch 5 batch 4 \n",
      "total loss: 2.2938127517700195 epoch 5 batch 6 \n",
      "total loss: 3.863168478012085 epoch 6 batch 2 \n",
      "total loss: 1.9003880023956299 epoch 6 batch 4 \n",
      "total loss: 0.14551769196987152 epoch 6 batch 6 \n",
      "total loss: 0.12727665901184082 epoch 7 batch 2 \n",
      "total loss: 0.11714531481266022 epoch 7 batch 4 \n",
      "total loss: 1.5440796613693237 epoch 7 batch 6 \n",
      "total loss: 1.5865687131881714 epoch 8 batch 2 \n",
      "total loss: 3.15659499168396 epoch 8 batch 4 \n",
      "total loss: 0.1168266013264656 epoch 8 batch 6 \n",
      "total loss: 0.11615722626447678 epoch 9 batch 2 \n",
      "total loss: 1.5927270650863647 epoch 9 batch 4 \n",
      "total loss: 0.11654993146657944 epoch 9 batch 6 \n",
      "total loss: 0.11341485381126404 epoch 10 batch 2 \n",
      "total loss: 1.4503698348999023 epoch 10 batch 4 \n",
      "total loss: 1.6362859010696411 epoch 10 batch 6 \n",
      "total loss: 1.4265124797821045 epoch 11 batch 2 \n",
      "total loss: 0.0998309999704361 epoch 11 batch 4 \n",
      "total loss: 1.6247416734695435 epoch 11 batch 6 \n",
      "total loss: 2.993894100189209 epoch 12 batch 2 \n",
      "total loss: 1.5493509769439697 epoch 12 batch 4 \n",
      "total loss: 1.6405390501022339 epoch 12 batch 6 \n",
      "total loss: 0.0927744135260582 epoch 13 batch 2 \n",
      "total loss: 1.6916003227233887 epoch 13 batch 4 \n",
      "total loss: 1.613573431968689 epoch 13 batch 6 \n",
      "total loss: 1.4496970176696777 epoch 14 batch 2 \n",
      "total loss: 1.6795117855072021 epoch 14 batch 4 \n",
      "total loss: 1.6669758558273315 epoch 14 batch 6 \n",
      "total loss: 1.4244029521942139 epoch 15 batch 2 \n",
      "total loss: 0.09274999052286148 epoch 15 batch 4 \n",
      "total loss: 1.5749980211257935 epoch 15 batch 6 \n",
      "total loss: 2.889418363571167 epoch 16 batch 2 \n",
      "total loss: 0.0897921696305275 epoch 16 batch 4 \n",
      "total loss: 1.6345648765563965 epoch 16 batch 6 \n",
      "total loss: 1.5019313097000122 epoch 17 batch 2 \n",
      "total loss: 1.6255223751068115 epoch 17 batch 4 \n",
      "total loss: 0.08125459402799606 epoch 17 batch 6 \n",
      "total loss: 0.07772665470838547 epoch 18 batch 2 \n",
      "total loss: 0.07621488720178604 epoch 18 batch 4 \n",
      "total loss: 2.9689152240753174 epoch 18 batch 6 \n",
      "total loss: 0.07329824566841125 epoch 19 batch 2 \n",
      "total loss: 1.5409339666366577 epoch 19 batch 4 \n",
      "total loss: 0.0769704282283783 epoch 19 batch 6 \n",
      "total loss: 3.0506062507629395 epoch 20 batch 2 \n",
      "total loss: 0.07555201649665833 epoch 20 batch 4 \n",
      "total loss: 2.9512672424316406 epoch 20 batch 6 \n",
      "total loss: 0.07130935788154602 epoch 21 batch 2 \n",
      "total loss: 1.5555295944213867 epoch 21 batch 4 \n",
      "total loss: 2.9603970050811768 epoch 21 batch 6 \n",
      "total loss: 1.6306129693984985 epoch 22 batch 2 \n",
      "total loss: 2.9924752712249756 epoch 22 batch 4 \n",
      "total loss: 1.3965342044830322 epoch 22 batch 6 \n",
      "total loss: 1.5299112796783447 epoch 23 batch 2 \n",
      "total loss: 0.06310027092695236 epoch 23 batch 4 \n",
      "total loss: 2.9503414630889893 epoch 23 batch 6 \n",
      "total loss: 1.3728840351104736 epoch 24 batch 2 \n",
      "total loss: 1.6254690885543823 epoch 24 batch 4 \n",
      "total loss: 0.06401406973600388 epoch 24 batch 6 \n",
      "total loss: 0.062027499079704285 epoch 25 batch 2 \n",
      "total loss: 1.507346749305725 epoch 25 batch 4 \n",
      "total loss: 1.6355719566345215 epoch 25 batch 6 \n",
      "total loss: 3.022145986557007 epoch 26 batch 2 \n",
      "total loss: 0.05886802449822426 epoch 26 batch 4 \n",
      "total loss: 1.561225175857544 epoch 26 batch 6 \n",
      "total loss: 1.5316786766052246 epoch 27 batch 2 \n",
      "total loss: 1.5166444778442383 epoch 27 batch 4 \n",
      "total loss: 0.05993936210870743 epoch 27 batch 6 \n",
      "total loss: 3.015120029449463 epoch 28 batch 2 \n",
      "total loss: 0.05814504623413086 epoch 28 batch 4 \n",
      "total loss: 1.537945032119751 epoch 28 batch 6 \n",
      "total loss: 1.5685724020004272 epoch 29 batch 2 \n",
      "total loss: 1.3750070333480835 epoch 29 batch 4 \n",
      "total loss: 0.0572931244969368 epoch 29 batch 6 \n",
      "total loss: 0.05543211102485657 epoch 30 batch 2 \n",
      "total loss: 1.5787954330444336 epoch 30 batch 4 \n",
      "total loss: 1.5739808082580566 epoch 30 batch 6 \n",
      "total loss: 1.4471479654312134 epoch 31 batch 2 \n",
      "total loss: 0.055292584002017975 epoch 31 batch 4 \n",
      "total loss: 2.9506044387817383 epoch 31 batch 6 \n",
      "total loss: 0.05478675290942192 epoch 32 batch 2 \n",
      "total loss: 0.05522238463163376 epoch 32 batch 4 \n",
      "total loss: 2.9102156162261963 epoch 32 batch 6 \n",
      "total loss: 1.4495824575424194 epoch 33 batch 2 \n",
      "total loss: 1.3877642154693604 epoch 33 batch 4 \n",
      "total loss: 0.057068999856710434 epoch 33 batch 6 \n",
      "total loss: 0.05519096925854683 epoch 34 batch 2 \n",
      "total loss: 0.05585640296339989 epoch 34 batch 4 \n",
      "total loss: 1.5144890546798706 epoch 34 batch 6 \n",
      "total loss: 1.3056997060775757 epoch 35 batch 2 \n",
      "total loss: 1.5570063591003418 epoch 35 batch 4 \n",
      "total loss: 1.5442640781402588 epoch 35 batch 6 \n",
      "total loss: 1.5320523977279663 epoch 36 batch 2 \n",
      "total loss: 1.514823317527771 epoch 36 batch 4 \n",
      "total loss: 1.4960777759552002 epoch 36 batch 6 \n",
      "total loss: 0.05351460352540016 epoch 37 batch 2 \n",
      "total loss: 1.3510518074035645 epoch 37 batch 4 \n",
      "total loss: 0.055488262325525284 epoch 37 batch 6 \n",
      "total loss: 2.970390796661377 epoch 38 batch 2 \n",
      "total loss: 0.05364914610981941 epoch 38 batch 4 \n",
      "total loss: 1.5323765277862549 epoch 38 batch 6 \n",
      "total loss: 1.438704490661621 epoch 39 batch 2 \n",
      "total loss: 0.0539863184094429 epoch 39 batch 4 \n",
      "total loss: 3.0807032585144043 epoch 39 batch 6 \n",
      "total loss: 2.9321532249450684 epoch 40 batch 2 \n",
      "total loss: 0.053371451795101166 epoch 40 batch 4 \n",
      "total loss: 2.7761216163635254 epoch 40 batch 6 \n",
      "total loss: 0.052924830466508865 epoch 41 batch 2 \n",
      "total loss: 1.4756921529769897 epoch 41 batch 4 \n",
      "total loss: 2.9472498893737793 epoch 41 batch 6 \n",
      "total loss: 1.3535962104797363 epoch 42 batch 2 \n",
      "total loss: 0.05469871312379837 epoch 42 batch 4 \n",
      "total loss: 1.4777638912200928 epoch 42 batch 6 \n",
      "total loss: 1.4700950384140015 epoch 43 batch 2 \n",
      "total loss: 0.05424797162413597 epoch 43 batch 4 \n",
      "total loss: 2.8255832195281982 epoch 43 batch 6 \n",
      "total loss: 2.9042763710021973 epoch 44 batch 2 \n",
      "total loss: 1.36105477809906 epoch 44 batch 4 \n",
      "total loss: 1.5638102293014526 epoch 44 batch 6 \n",
      "total loss: 2.935190439224243 epoch 45 batch 2 \n",
      "total loss: 0.054108504205942154 epoch 45 batch 4 \n",
      "total loss: 1.4262655973434448 epoch 45 batch 6 \n",
      "total loss: 0.05186563730239868 epoch 46 batch 2 \n",
      "total loss: 1.552917242050171 epoch 46 batch 4 \n",
      "total loss: 1.5560920238494873 epoch 46 batch 6 \n",
      "total loss: 1.5422741174697876 epoch 47 batch 2 \n",
      "total loss: 1.5407466888427734 epoch 47 batch 4 \n",
      "total loss: 1.4496296644210815 epoch 47 batch 6 \n",
      "total loss: 1.2976809740066528 epoch 48 batch 2 \n",
      "total loss: 0.05207306146621704 epoch 48 batch 4 \n",
      "total loss: 2.9303722381591797 epoch 48 batch 6 \n",
      "total loss: 0.0499715618789196 epoch 49 batch 2 \n",
      "total loss: 1.4806081056594849 epoch 49 batch 4 \n",
      "total loss: 1.4357191324234009 epoch 49 batch 6 \n",
      "total loss: 1.532629370689392 epoch 50 batch 2 \n",
      "total loss: 2.8926587104797363 epoch 50 batch 4 \n",
      "total loss: 0.054321903735399246 epoch 50 batch 6 \n",
      "total loss: 1.3561334609985352 epoch 51 batch 2 \n",
      "total loss: 1.492483377456665 epoch 51 batch 4 \n",
      "total loss: 1.5614378452301025 epoch 51 batch 6 \n",
      "total loss: 1.4195314645767212 epoch 52 batch 2 \n",
      "total loss: 1.5509463548660278 epoch 52 batch 4 \n",
      "total loss: 2.8786232471466064 epoch 52 batch 6 \n",
      "total loss: 1.437145471572876 epoch 53 batch 2 \n",
      "total loss: 1.3550364971160889 epoch 53 batch 4 \n",
      "total loss: 1.5452594757080078 epoch 53 batch 6 \n",
      "total loss: 1.415398359298706 epoch 54 batch 2 \n",
      "total loss: 0.051880452781915665 epoch 54 batch 4 \n",
      "total loss: 0.05288580060005188 epoch 54 batch 6 \n",
      "total loss: 1.4546177387237549 epoch 55 batch 2 \n",
      "total loss: 2.928725481033325 epoch 55 batch 4 \n",
      "total loss: 0.05177828669548035 epoch 55 batch 6 \n",
      "total loss: 0.04953555390238762 epoch 56 batch 2 \n",
      "total loss: 1.4407694339752197 epoch 56 batch 4 \n",
      "total loss: 0.0521293506026268 epoch 56 batch 6 \n",
      "total loss: 1.4107943773269653 epoch 57 batch 2 \n",
      "total loss: 1.3270676136016846 epoch 57 batch 4 \n",
      "total loss: 0.050532493740320206 epoch 57 batch 6 \n",
      "total loss: 1.4925556182861328 epoch 58 batch 2 \n",
      "total loss: 1.4312376976013184 epoch 58 batch 4 \n",
      "total loss: 1.5170443058013916 epoch 58 batch 6 \n",
      "total loss: 1.4189467430114746 epoch 59 batch 2 \n",
      "total loss: 1.510711669921875 epoch 59 batch 4 \n",
      "total loss: 0.04951661825180054 epoch 59 batch 6 \n",
      "total loss: 1.4865666627883911 epoch 60 batch 2 \n",
      "total loss: 2.7519419193267822 epoch 60 batch 4 \n",
      "total loss: 1.4438992738723755 epoch 60 batch 6 \n",
      "total loss: 1.4406410455703735 epoch 61 batch 2 \n",
      "total loss: 0.049579888582229614 epoch 61 batch 4 \n",
      "total loss: 1.336768627166748 epoch 61 batch 6 \n",
      "total loss: 0.04784423112869263 epoch 62 batch 2 \n",
      "total loss: 0.04750949144363403 epoch 62 batch 4 \n",
      "total loss: 2.9188852310180664 epoch 62 batch 6 \n",
      "total loss: 1.4382978677749634 epoch 63 batch 2 \n",
      "total loss: 1.4209809303283691 epoch 63 batch 4 \n",
      "total loss: 1.3188457489013672 epoch 63 batch 6 \n",
      "total loss: 1.415499210357666 epoch 64 batch 2 \n",
      "total loss: 1.414257526397705 epoch 64 batch 4 \n",
      "total loss: 0.047414377331733704 epoch 64 batch 6 \n",
      "total loss: 1.3017637729644775 epoch 65 batch 2 \n",
      "total loss: 1.5074163675308228 epoch 65 batch 4 \n",
      "total loss: 2.8627798557281494 epoch 65 batch 6 \n",
      "total loss: 1.277801752090454 epoch 66 batch 2 \n",
      "total loss: 2.873392343521118 epoch 66 batch 4 \n",
      "total loss: 1.4917017221450806 epoch 66 batch 6 \n",
      "total loss: 0.044753313064575195 epoch 67 batch 2 \n",
      "total loss: 1.3855106830596924 epoch 67 batch 4 \n",
      "total loss: 1.4840577840805054 epoch 67 batch 6 \n",
      "total loss: 0.04410620406270027 epoch 68 batch 2 \n",
      "total loss: 2.7955665588378906 epoch 68 batch 4 \n",
      "total loss: 0.04586411267518997 epoch 68 batch 6 \n",
      "total loss: 2.752875328063965 epoch 69 batch 2 \n",
      "total loss: 0.04632856696844101 epoch 69 batch 4 \n",
      "total loss: 1.4411108493804932 epoch 69 batch 6 \n",
      "total loss: 0.04408210888504982 epoch 70 batch 2 \n",
      "total loss: 2.7170605659484863 epoch 70 batch 4 \n",
      "total loss: 1.4307847023010254 epoch 70 batch 6 \n",
      "total loss: 1.4933668375015259 epoch 71 batch 2 \n",
      "total loss: 1.411744236946106 epoch 71 batch 4 \n",
      "total loss: 2.762187957763672 epoch 71 batch 6 \n",
      "total loss: 2.7123522758483887 epoch 72 batch 2 \n",
      "total loss: 1.3001903295516968 epoch 72 batch 4 \n",
      "total loss: 0.043552983552217484 epoch 72 batch 6 \n",
      "total loss: 1.4786603450775146 epoch 73 batch 2 \n",
      "total loss: 0.04231439158320427 epoch 73 batch 4 \n",
      "total loss: 1.465004324913025 epoch 73 batch 6 \n",
      "total loss: 2.6557857990264893 epoch 74 batch 2 \n",
      "total loss: 0.04069063067436218 epoch 74 batch 4 \n",
      "total loss: 2.7130579948425293 epoch 74 batch 6 \n",
      "total loss: 0.039422232657670975 epoch 75 batch 2 \n",
      "total loss: 2.8481245040893555 epoch 75 batch 4 \n",
      "total loss: 2.6687991619110107 epoch 75 batch 6 \n",
      "total loss: 0.03931649774312973 epoch 76 batch 2 \n",
      "total loss: 0.03997643291950226 epoch 76 batch 4 \n",
      "total loss: 1.4354594945907593 epoch 76 batch 6 \n",
      "total loss: 1.4237724542617798 epoch 77 batch 2 \n",
      "total loss: 2.8074123859405518 epoch 77 batch 4 \n",
      "total loss: 0.04074011370539665 epoch 77 batch 6 \n",
      "total loss: 1.3524243831634521 epoch 78 batch 2 \n",
      "total loss: 1.394849181175232 epoch 78 batch 4 \n",
      "total loss: 0.03903995454311371 epoch 78 batch 6 \n",
      "total loss: 1.3871742486953735 epoch 79 batch 2 \n",
      "total loss: 1.388049840927124 epoch 79 batch 4 \n",
      "total loss: 2.6874070167541504 epoch 79 batch 6 \n",
      "total loss: 1.2749260663986206 epoch 80 batch 2 \n",
      "total loss: 1.463132381439209 epoch 80 batch 4 \n",
      "total loss: 0.03864418715238571 epoch 80 batch 6 \n",
      "total loss: 0.03781326115131378 epoch 81 batch 2 \n",
      "total loss: 1.3329282999038696 epoch 81 batch 4 \n",
      "total loss: 1.2692584991455078 epoch 81 batch 6 \n",
      "total loss: 0.037100739777088165 epoch 82 batch 2 \n",
      "total loss: 2.7521095275878906 epoch 82 batch 4 \n",
      "total loss: 1.4150735139846802 epoch 82 batch 6 \n",
      "total loss: 0.03723493590950966 epoch 83 batch 2 \n",
      "total loss: 1.4253904819488525 epoch 83 batch 4 \n",
      "total loss: 1.352845311164856 epoch 83 batch 6 \n",
      "total loss: 0.0376780740916729 epoch 84 batch 2 \n",
      "total loss: 1.3583719730377197 epoch 84 batch 4 \n",
      "total loss: 0.03790374472737312 epoch 84 batch 6 \n",
      "total loss: 2.5345442295074463 epoch 85 batch 2 \n",
      "total loss: 0.03672671318054199 epoch 85 batch 4 \n",
      "total loss: 0.03818847984075546 epoch 85 batch 6 \n",
      "total loss: 2.6111507415771484 epoch 86 batch 2 \n",
      "total loss: 0.03787825629115105 epoch 86 batch 4 \n",
      "total loss: 1.337249755859375 epoch 86 batch 6 \n",
      "total loss: 2.6339030265808105 epoch 87 batch 2 \n",
      "total loss: 1.4080218076705933 epoch 87 batch 4 \n",
      "total loss: 1.2620220184326172 epoch 87 batch 6 \n",
      "total loss: 2.480834484100342 epoch 88 batch 2 \n",
      "total loss: 0.03643510118126869 epoch 88 batch 4 \n",
      "total loss: 1.3551760911941528 epoch 88 batch 6 \n",
      "total loss: 2.412180185317993 epoch 89 batch 2 \n",
      "total loss: 1.4049429893493652 epoch 89 batch 4 \n",
      "total loss: 1.3794125318527222 epoch 89 batch 6 \n",
      "total loss: 1.1868021488189697 epoch 90 batch 2 \n",
      "total loss: 2.5596494674682617 epoch 90 batch 4 \n",
      "total loss: 0.03657222166657448 epoch 90 batch 6 \n",
      "total loss: 1.3346149921417236 epoch 91 batch 2 \n",
      "total loss: 2.593574047088623 epoch 91 batch 4 \n",
      "total loss: 0.036125313490629196 epoch 91 batch 6 \n",
      "total loss: 0.0359945073723793 epoch 92 batch 2 \n",
      "total loss: 1.3334094285964966 epoch 92 batch 4 \n",
      "total loss: 0.03563624620437622 epoch 92 batch 6 \n",
      "total loss: 0.03400590643286705 epoch 93 batch 2 \n",
      "total loss: 1.194088339805603 epoch 93 batch 4 \n",
      "total loss: 2.630821466445923 epoch 93 batch 6 \n",
      "total loss: 0.033499084413051605 epoch 94 batch 2 \n",
      "total loss: 1.4000542163848877 epoch 94 batch 4 \n",
      "total loss: 1.3462929725646973 epoch 94 batch 6 \n",
      "total loss: 1.377515435218811 epoch 95 batch 2 \n",
      "total loss: 0.03652837127447128 epoch 95 batch 4 \n",
      "total loss: 1.3695263862609863 epoch 95 batch 6 \n",
      "total loss: 2.6441545486450195 epoch 96 batch 2 \n",
      "total loss: 1.1373368501663208 epoch 96 batch 4 \n",
      "total loss: 0.03573795035481453 epoch 96 batch 6 \n",
      "total loss: 1.2691093683242798 epoch 97 batch 2 \n",
      "total loss: 0.03538023307919502 epoch 97 batch 4 \n",
      "total loss: 1.2373480796813965 epoch 97 batch 6 \n",
      "total loss: 1.1047441959381104 epoch 98 batch 2 \n",
      "total loss: 0.03596780449151993 epoch 98 batch 4 \n",
      "total loss: 2.6100552082061768 epoch 98 batch 6 \n",
      "total loss: 1.334206223487854 epoch 99 batch 2 \n",
      "total loss: 2.4276247024536133 epoch 99 batch 4 \n",
      "total loss: 0.03545152395963669 epoch 99 batch 6 \n",
      "total loss: 0.03307632729411125 epoch 100 batch 2 \n",
      "total loss: 2.3732008934020996 epoch 100 batch 4 \n",
      "total loss: 1.1293808221817017 epoch 100 batch 6 \n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(1, epochs+1):\n",
    "\n",
    "    encoder_initial_cell_state = initialize_initial_state()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
    "        total_loss += batch_loss\n",
    "        if (batch+1)%2 == 0:\n",
    "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#In this section we evaluate our model on a raw_input converted to german, for this the entire sentence has to be passed\n",
    "#through the length of the model, for this we use greedsampler to run through the decoder\n",
    "#and the final embedding matrix trained on the data is used to generate embeddings\n",
    "input_raw=X_test[0].reshape(1,-1)\n",
    "# inp_bodytext = nlp(input_raw)\n",
    "# input_lines = clean_text(inp_bodytext)\n",
    "\n",
    "# We have a transcript file containing English-German pairs\n",
    "# Preprocess X\n",
    "#input_raw = clean_text(input_raw)\n",
    "#input_lines = [f'{bt_tok} {input_raw}']\n",
    "# input_sequences = [[bt_tokenizer.word_index[w] for w in line.split()] for line in input_raw]\n",
    "# input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_raw,\n",
    "#                                                                maxlen=longest_seq, padding='post')\n",
    "# inp = tf.convert_to_tensor(input_sequences)\n",
    "\n",
    "#print(\"inp\", inp.shape)\n",
    "#print(\"inp_seq\",input_sequences)\n",
    "#inference_batch_size = input_sequences.shape[0]\n",
    "encoder_initial_cell_state = [tf.zeros((1, rnn_units)),tf.zeros((1, rnn_units))]\n",
    "encoder_emb_inp = encoderNetwork.encoder_embedding(input_raw)\n",
    "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,initial_state =encoder_initial_cell_state)\n",
    "# print('a_tx :', a_tx.shape)\n",
    "# print('c_tx :', c_tx.shape)\n",
    "\n",
    "start_tokens = tf.fill([1],bt_tokenizer.word_index['<start>'])\n",
    "\n",
    "end_token = bt_tokenizer.word_index['<end>']\n",
    "\n",
    "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\n",
    "decoder_input = tf.expand_dims([bt_tokenizer.word_index['<start>']]* 1,1)\n",
    "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
    "                                            output_layer=decoderNetwork.dense_layer)\n",
    "decoderNetwork.attention_mechanism.setup_memory(c_tx)\n",
    "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
    "# print(f\"decoder_initial_state = [a_tx, c_tx] : {np.array([a_tx, c_tx]).shape}\")\n",
    "decoder_initial_state = decoderNetwork.build_decoder_initial_state(1,\n",
    "                                                                   encoder_state=[a_tx, c_tx],\n",
    "                                                                   Dtype=tf.float32)\n",
    "# print(f\"\"\"\n",
    "# Compared to simple encoder-decoder without attention, the decoder_initial_state\n",
    "# is an AttentionWrapperState object containing s_prev tensors and context and alignment vector\n",
    "\n",
    "# decoder initial state shape: {np.array(decoder_initial_state).shape}\n",
    "# decoder_initial_state tensor\n",
    "# {decoder_initial_state}\n",
    "# \"\"\")\n",
    "\n",
    "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
    "# One heuristic is to decode up to two times the source sentence lengths.\n",
    "maximum_iterations = tf.round(tf.reduce_max(longest_seq)*2)\n",
    "\n",
    "#initialize inference decoder\n",
    "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
    "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
    "                             start_tokens = start_tokens,\n",
    "                             end_token=end_token,\n",
    "                             initial_state = decoder_initial_state)\n",
    "#print( first_finished.shape)\n",
    "#print(f\"first_inputs returns the same decoder_input i.e. embedding of  {'<start>'} : {first_inputs.shape}\")\n",
    "#print(f\"start_index_emb_avg {tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))}\") # mean along the batch\n",
    "\n",
    "inputs = first_inputs\n",
    "state = first_state \n",
    "\n",
    "predictions = np.empty((1,0), dtype = np.int32)                                                                             \n",
    "for j in range(maximum_iterations):\n",
    "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
    "    inputs = next_inputs\n",
    "    state = next_state\n",
    "    #print(next_inputs)\n",
    "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "    predictions = np.append(predictions, outputs, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "<end> <end> <end> <end> <end> <end> <end> the <end> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSummary:\")\n",
    "for i in range(len(predictions)):\n",
    "    line = predictions[i,:]\n",
    "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
    "    print(\" \".join( [bt_tokenizer.index_word[w] for w in seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_bt</th>\n",
       "      <th>len_ab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>70.008200</td>\n",
       "      <td>20.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.715234</td>\n",
       "      <td>23.949265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             len_bt        len_ab\n",
       "count  10000.000000  10000.000000\n",
       "mean      70.008200     20.265300\n",
       "std        1.715234     23.949265\n",
       "min        1.000000      1.000000\n",
       "25%       70.000000      1.000000\n",
       "50%       70.000000      1.000000\n",
       "75%       70.000000     50.000000\n",
       "max       83.000000     53.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_latest_p37)",
   "language": "python",
   "name": "conda_tensorflow2_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
